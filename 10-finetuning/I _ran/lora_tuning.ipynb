{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"accelerator":"GPU","colab":{"name":"lora_tuning.ipynb","toc_visible":true,"provenance":[]},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceType":"modelInstanceVersion","sourceId":316991,"databundleVersionId":11657607,"modelInstanceId":257756}],"dockerImageVersionId":31090,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"<link rel=\"stylesheet\" href=\"/site-assets/css/gemma.css\">\n<link rel=\"stylesheet\" href=\"https://fonts.googleapis.com/css2?family=Google+Symbols:opsz,wght,FILL,GRAD@20..48,100..700,0..1,-50..200\" />","metadata":{"id":"G3MMAcssHTML"}},{"cell_type":"markdown","source":"##### Copyright 2025 Google LLC.","metadata":{"id":"Tce3stUlHN0L"}},{"cell_type":"code","source":"#@title Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# https://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.","metadata":{"cellView":"form","id":"tuOe1ymfHZPu","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Fine-tune Gemma in Keras using LoRA","metadata":{"id":"SDEExiAk4fLb"}},{"cell_type":"markdown","source":"<table class=\"tfo-notebook-buttons\" align=\"left\">\n  <td>\n    <a target=\"_blank\" href=\"https://ai.google.dev/gemma/docs/core/lora_tuning\"><img src=\"https://ai.google.dev/static/site-assets/images/docs/notebook-site-button.png\" height=\"32\" width=\"32\" />View on ai.google.dev</a>\n  <td>\n    <a target=\"_blank\" href=\"https://colab.research.google.com/github/google/generative-ai-docs/blob/main/site/en/gemma/docs/core/lora_tuning.ipynb\"><img src=\"https://www.tensorflow.org/images/colab_logo_32px.png\" />Run in Google Colab</a>\n  </td>\n  <td>\n    <a target=\"_blank\" href=\"https://console.cloud.google.com/vertex-ai/workbench/deploy-notebook?download_url=https://raw.githubusercontent.com/google/generative-ai-docs/main/site/en/gemma/docs/core/lora_tuning.ipynb\"><img src=\"https://ai.google.dev/images/cloud-icon.svg\" width=\"40\" />Open in Vertex AI</a>\n  </td>\n  <td>\n    <a target=\"_blank\" href=\"https://github.com/google/generative-ai-docs/blob/main/site/en/gemma/docs/core/lora_tuning.ipynb\"><img src=\"https://www.tensorflow.org/images/GitHub-Mark-32px.png\" />View source on GitHub</a>\n  </td>\n</table>","metadata":{"id":"ZFWzQEqNosrS"}},{"cell_type":"markdown","source":"Generative artificial intelligent (AI) models like Gemma are effective at a variety of tasks. You can further fine-tune Gemma models with domain-specific data to perform tasks such as sentiment analysis. However, full fine-tuning of generative models by updating billions of parameters is resource intensive, requiring specialized hardware, such as GPUs, processing time, and memory to load the model parameters.\n\n[Low Rank Adaptation](https://arxiv.org/abs/2106.09685) (LoRA) is a fine-tuning technique which greatly reduces the number of trainable parameters for downstream tasks by freezing the weights of the model and inserting a smaller number of new weights into the model. This technique makes training with LoRA much faster and more memory-efficient, and produces smaller model weights (a few hundred MBs), all while maintaining the quality of the model outputs. This tutorial walks you through using Keras to perform LoRA fine-tuning on a Gemma model.","metadata":{"id":"lSGRSsRPgkzK"}},{"cell_type":"markdown","source":"## Setup\n\nTo complete this tutorial, you will first need to complete the setup instructions at [Gemma setup](https://ai.google.dev/gemma/docs/setup). The Gemma setup instructions show you how to do the following:\n\n* Get access to Gemma on [kaggle.com](https://kaggle.com).\n* Select a Colab runtime with sufficient resources to tune\n  the Gemma model you want to run. [Learn more](https://ai.google.dev/gemma/docs/core#sizes).\n* Generate and configure a Kaggle username and API key.\n\nAfter you've completed the Gemma setup, move on to the next section, where you'll set environment variables for your Colab environment.","metadata":{"id":"lyhHCMfoRZ_v"}},{"cell_type":"markdown","source":"### Select a Colab runtime\n\nTo complete this tutorial, you'll need to have a Colab runtime with sufficient resources to run the Gemma model. In this case, you can use a T4 GPU:\n\n1. In the upper-right of the Colab window, select &#9662; (**Additional connection options**).\n2. Select **Change runtime type**.\n3. Under **Hardware accelerator**, select **T4 GPU**.","metadata":{"id":"AZ5Qo0fxRZ1V"}},{"cell_type":"markdown","source":"### Configure your API key\n\nTo use Gemma, you must provide your Kaggle username and a Kaggle API key.\n\nTo generate a Kaggle API key, go to the **Account** tab of your Kaggle user profile and select **Create New Token**. This triggers the download of a `kaggle.json` file containing your API credentials.\n\nIn Colab, select **Secrets** (🔑) in the left pane and add your Kaggle username and Kaggle API key. Store your username under the name `KAGGLE_USERNAME` and your API key under the name `KAGGLE_KEY`.","metadata":{"id":"hsPC0HRkJl0K"}},{"cell_type":"markdown","source":"### Set environment variables\n\nSet environment variables for `KAGGLE_USERNAME` and `KAGGLE_KEY`.","metadata":{"id":"7iOF6Yo-wUEC"}},{"cell_type":"code","source":"import os\n#from google.colab import userdata\n\n# Note: `userdata.get` is a Colab API. If you're not using Colab, set the env\n# vars as appropriate for your system.\n\nos.environ[\"KAGGLE_USERNAME\"] = userdata.get('KAGGLE_USERNAME')\nos.environ[\"KAGGLE_KEY\"] = userdata.get('KAGGLE_KEY')","metadata":{"id":"0_EdOg9DPK6Q","trusted":true,"execution":{"iopub.status.busy":"2025-07-11T03:02:47.509617Z","iopub.execute_input":"2025-07-11T03:02:47.510332Z","iopub.status.idle":"2025-07-11T03:02:47.577487Z","shell.execute_reply.started":"2025-07-11T03:02:47.510308Z","shell.execute_reply":"2025-07-11T03:02:47.576508Z"}},"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_187/3189849711.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# vars as appropriate for your system.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menviron\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"KAGGLE_USERNAME\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0muserdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'KAGGLE_USERNAME'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menviron\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"KAGGLE_KEY\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0muserdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'KAGGLE_KEY'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'userdata' is not defined"],"ename":"NameError","evalue":"name 'userdata' is not defined","output_type":"error"}],"execution_count":1},{"cell_type":"markdown","source":"### Install Keras packages\n\nInstall the Keras and KerasHub Python packages.","metadata":{"id":"CuEUAKJW1QkQ"}},{"cell_type":"code","source":"!pip install -q -U keras-hub\n!pip install  -q -U keras","metadata":{"id":"1eeBtYqJsZPG","trusted":true,"execution":{"iopub.status.busy":"2025-07-11T03:02:51.539030Z","iopub.execute_input":"2025-07-11T03:02:51.539628Z","iopub.status.idle":"2025-07-11T03:02:58.542780Z","shell.execute_reply.started":"2025-07-11T03:02:51.539595Z","shell.execute_reply":"2025-07-11T03:02:58.541942Z"}},"outputs":[{"name":"stdout","text":"\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nkeras-nlp 0.18.1 requires keras-hub==0.18.1, but you have keras-hub 0.21.1 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0m","output_type":"stream"}],"execution_count":2},{"cell_type":"markdown","source":"### Select a backend\n\nKeras is a high-level, multi-framework deep learning API designed for simplicity and ease of use. Using Keras 3, you can run workflows on one of three backends: TensorFlow, JAX, or PyTorch. For this tutorial, configure the backend for JAX as it typically provides the better performance.","metadata":{"id":"rGLS-l5TxIR4"}},{"cell_type":"code","source":"os.environ[\"KERAS_BACKEND\"] = \"jax\"  # Or \"torch\" or \"tensorflow\".\n# Avoid memory fragmentation on JAX backend.\nos.environ[\"XLA_PYTHON_CLIENT_MEM_FRACTION\"]=\"1.00\"","metadata":{"id":"yn5uy8X8sdD0","trusted":true,"execution":{"iopub.status.busy":"2025-07-11T03:02:58.544412Z","iopub.execute_input":"2025-07-11T03:02:58.544983Z","iopub.status.idle":"2025-07-11T03:02:58.548992Z","shell.execute_reply.started":"2025-07-11T03:02:58.544956Z","shell.execute_reply":"2025-07-11T03:02:58.548335Z"}},"outputs":[],"execution_count":3},{"cell_type":"markdown","source":"### Import packages\n\nImport the Python packages needed for this tutorial, including Keras and KerasHub.","metadata":{"id":"hZs8XXqUKRmi"}},{"cell_type":"code","source":"import keras\nimport keras_hub","metadata":{"id":"FYHyPUA9hKTf","trusted":true,"execution":{"iopub.status.busy":"2025-07-11T03:02:58.549665Z","iopub.execute_input":"2025-07-11T03:02:58.549892Z","iopub.status.idle":"2025-07-11T03:03:02.836251Z","shell.execute_reply.started":"2025-07-11T03:02:58.549868Z","shell.execute_reply":"2025-07-11T03:03:02.835666Z"}},"outputs":[{"name":"stderr","text":"2025-07-11 03:03:00.414458: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1752202980.436496     187 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1752202980.443198     187 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"}],"execution_count":4},{"cell_type":"markdown","source":"## Load model\n\nKeras provides implementations of Gemma and many other popular [model architectures](https://keras.io/keras_hub/api/models/). Use the `Gemma3CausalLM.from_preset()` method to configure an end-to-end Gemma model for causal language modeling. A causal language model predicts the next token based on previous tokens.","metadata":{"id":"7RCE3fdGhDE5"}},{"cell_type":"code","source":"gemma_lm = keras_hub.models.Gemma3CausalLM.from_preset(\"gemma3_1b\")\ngemma_lm.summary()","metadata":{"id":"vz5zLEyLstfn","trusted":true,"execution":{"iopub.status.busy":"2025-07-11T03:03:02.837707Z","iopub.execute_input":"2025-07-11T03:03:02.838349Z","iopub.status.idle":"2025-07-11T03:03:22.544961Z","shell.execute_reply.started":"2025-07-11T03:03:02.838320Z","shell.execute_reply":"2025-07-11T03:03:22.544371Z"}},"outputs":[{"name":"stderr","text":"INFO:2025-07-11 03:03:04,906:jax._src.xla_bridge:924: Unable to initialize backend 'rocm': module 'jaxlib.xla_extension' has no attribute 'GpuAllocatorConfig'\nINFO:2025-07-11 03:03:04,908:jax._src.xla_bridge:924: Unable to initialize backend 'tpu': INTERNAL: Failed to open libtpu.so: libtpu.so: cannot open shared object file: No such file or directory\nI0000 00:00:1752202988.080143     187 gpu_device.cc:2022] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 1173 MB memory:  -> device: 0, name: Tesla T4, pci bus id: 0000:00:04.0, compute capability: 7.5\nI0000 00:00:1752202988.080768     187 gpu_device.cc:2022] Created device /job:localhost/replica:0/task:0/device:GPU:1 with 13940 MB memory:  -> device: 1, name: Tesla T4, pci bus id: 0000:00:05.0, compute capability: 7.5\nnormalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"\u001b[1mPreprocessor: \"gemma3_causal_lm_preprocessor\"\u001b[0m\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Preprocessor: \"gemma3_causal_lm_preprocessor\"</span>\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                                                 \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m                                  Config\u001b[0m\u001b[1m \u001b[0m┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n│ gemma3_tokenizer (\u001b[38;5;33mGemma3Tokenizer\u001b[0m)                            │                      Vocab size: \u001b[38;5;34m262,144\u001b[0m │\n└───────────────────────────────────────────────────────────────┴──────────────────────────────────────────┘\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n┃<span style=\"font-weight: bold\"> Layer (type)                                                  </span>┃<span style=\"font-weight: bold\">                                   Config </span>┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n│ gemma3_tokenizer (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Gemma3Tokenizer</span>)                            │                      Vocab size: <span style=\"color: #00af00; text-decoration-color: #00af00\">262,144</span> │\n└───────────────────────────────────────────────────────────────┴──────────────────────────────────────────┘\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1mModel: \"gemma3_causal_lm\"\u001b[0m\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"gemma3_causal_lm\"</span>\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                 \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape             \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mConnected to              \u001b[0m\u001b[1m \u001b[0m┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n│ padding_mask (\u001b[38;5;33mInputLayer\u001b[0m)     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m)              │               \u001b[38;5;34m0\u001b[0m │ -                          │\n├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n│ token_ids (\u001b[38;5;33mInputLayer\u001b[0m)        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m)              │               \u001b[38;5;34m0\u001b[0m │ -                          │\n├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n│ gemma3_backbone               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1152\u001b[0m)        │     \u001b[38;5;34m999,885,952\u001b[0m │ padding_mask[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],        │\n│ (\u001b[38;5;33mGemma3Backbone\u001b[0m)              │                           │                 │ token_ids[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]            │\n├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n│ token_embedding               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m262144\u001b[0m)      │     \u001b[38;5;34m301,989,888\u001b[0m │ gemma3_backbone[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]      │\n│ (\u001b[38;5;33mReversibleEmbedding\u001b[0m)         │                           │                 │                            │\n└───────────────────────────────┴───────────────────────────┴─────────────────┴────────────────────────────┘\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n┃<span style=\"font-weight: bold\"> Layer (type)                  </span>┃<span style=\"font-weight: bold\"> Output Shape              </span>┃<span style=\"font-weight: bold\">         Param # </span>┃<span style=\"font-weight: bold\"> Connected to               </span>┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n│ padding_mask (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>)              │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                          │\n├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n│ token_ids (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>)              │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                          │\n├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n│ gemma3_backbone               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1152</span>)        │     <span style=\"color: #00af00; text-decoration-color: #00af00\">999,885,952</span> │ padding_mask[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],        │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Gemma3Backbone</span>)              │                           │                 │ token_ids[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]            │\n├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n│ token_embedding               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">262144</span>)      │     <span style=\"color: #00af00; text-decoration-color: #00af00\">301,989,888</span> │ gemma3_backbone[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]      │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">ReversibleEmbedding</span>)         │                           │                 │                            │\n└───────────────────────────────┴───────────────────────────┴─────────────────┴────────────────────────────┘\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Total params: \u001b[0m\u001b[38;5;34m999,885,952\u001b[0m (3.72 GB)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">999,885,952</span> (3.72 GB)\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m999,885,952\u001b[0m (3.72 GB)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">999,885,952</span> (3.72 GB)\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n</pre>\n"},"metadata":{}}],"execution_count":5},{"cell_type":"markdown","source":"The `Gemma3CausalLM.from_preset()` method instantiates the model from a preset architecture and weights. In the code above, the string `\"gemma#_xxxxxxx\"` specifies a preset version and parameter size for Gemma. You can find the code strings for Gemma models in their **Model Variation** listings on [Kaggle](https://www.kaggle.com/models/keras/gemma3).","metadata":{"id":"Nl4lvPy5zA26"}},{"cell_type":"markdown","source":"## Inference before fine tuning\n\nOnce you have downloaded and configured a Gemma model, you can query it with various prompts to see how it responds.","metadata":{"id":"G_L6A5J-1QgC"}},{"cell_type":"markdown","source":"### Europe trip prompt\n\nQuery the model for suggestions on what to do on a trip to Europe.","metadata":{"id":"PVLXadptyo34"}},{"cell_type":"code","source":"pip install keras_nlp","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-11T03:03:22.545594Z","iopub.execute_input":"2025-07-11T03:03:22.545822Z","iopub.status.idle":"2025-07-11T03:03:26.311839Z","shell.execute_reply.started":"2025-07-11T03:03:22.545804Z","shell.execute_reply":"2025-07-11T03:03:26.311014Z"}},"outputs":[{"name":"stderr","text":"/usr/lib/python3.11/pty.py:89: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  pid, fd = os.forkpty()\n","output_type":"stream"},{"name":"stdout","text":"Requirement already satisfied: keras_nlp in /usr/local/lib/python3.11/dist-packages (0.18.1)\nCollecting keras-hub==0.18.1 (from keras_nlp)\n  Using cached keras_hub-0.18.1-py3-none-any.whl.metadata (7.0 kB)\nRequirement already satisfied: absl-py in /usr/local/lib/python3.11/dist-packages (from keras-hub==0.18.1->keras_nlp) (1.4.0)\nRequirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from keras-hub==0.18.1->keras_nlp) (1.26.4)\nRequirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from keras-hub==0.18.1->keras_nlp) (25.0)\nRequirement already satisfied: regex in /usr/local/lib/python3.11/dist-packages (from keras-hub==0.18.1->keras_nlp) (2024.11.6)\nRequirement already satisfied: rich in /usr/local/lib/python3.11/dist-packages (from keras-hub==0.18.1->keras_nlp) (14.0.0)\nRequirement already satisfied: kagglehub in /usr/local/lib/python3.11/dist-packages (from keras-hub==0.18.1->keras_nlp) (0.3.12)\nRequirement already satisfied: tensorflow-text in /usr/local/lib/python3.11/dist-packages (from keras-hub==0.18.1->keras_nlp) (2.18.1)\nRequirement already satisfied: pyyaml in /usr/local/lib/python3.11/dist-packages (from kagglehub->keras-hub==0.18.1->keras_nlp) (6.0.2)\nRequirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from kagglehub->keras-hub==0.18.1->keras_nlp) (2.32.4)\nRequirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from kagglehub->keras-hub==0.18.1->keras_nlp) (4.67.1)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy->keras-hub==0.18.1->keras_nlp) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy->keras-hub==0.18.1->keras_nlp) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy->keras-hub==0.18.1->keras_nlp) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy->keras-hub==0.18.1->keras_nlp) (2025.2.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy->keras-hub==0.18.1->keras_nlp) (2022.2.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy->keras-hub==0.18.1->keras_nlp) (2.4.1)\nRequirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras-hub==0.18.1->keras_nlp) (3.0.0)\nRequirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras-hub==0.18.1->keras_nlp) (2.19.2)\nRequirement already satisfied: tensorflow<2.19,>=2.18.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow-text->keras-hub==0.18.1->keras_nlp) (2.18.0)\nRequirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich->keras-hub==0.18.1->keras_nlp) (0.1.2)\nRequirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow<2.19,>=2.18.0->tensorflow-text->keras-hub==0.18.1->keras_nlp) (1.6.3)\nRequirement already satisfied: flatbuffers>=24.3.25 in /usr/local/lib/python3.11/dist-packages (from tensorflow<2.19,>=2.18.0->tensorflow-text->keras-hub==0.18.1->keras_nlp) (25.2.10)\nRequirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow<2.19,>=2.18.0->tensorflow-text->keras-hub==0.18.1->keras_nlp) (0.6.0)\nRequirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow<2.19,>=2.18.0->tensorflow-text->keras-hub==0.18.1->keras_nlp) (0.2.0)\nRequirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow<2.19,>=2.18.0->tensorflow-text->keras-hub==0.18.1->keras_nlp) (18.1.1)\nRequirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.11/dist-packages (from tensorflow<2.19,>=2.18.0->tensorflow-text->keras-hub==0.18.1->keras_nlp) (3.4.0)\nRequirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow<2.19,>=2.18.0->tensorflow-text->keras-hub==0.18.1->keras_nlp) (3.20.3)\nRequirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from tensorflow<2.19,>=2.18.0->tensorflow-text->keras-hub==0.18.1->keras_nlp) (75.2.0)\nRequirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow<2.19,>=2.18.0->tensorflow-text->keras-hub==0.18.1->keras_nlp) (1.17.0)\nRequirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow<2.19,>=2.18.0->tensorflow-text->keras-hub==0.18.1->keras_nlp) (3.1.0)\nRequirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.11/dist-packages (from tensorflow<2.19,>=2.18.0->tensorflow-text->keras-hub==0.18.1->keras_nlp) (4.14.0)\nRequirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow<2.19,>=2.18.0->tensorflow-text->keras-hub==0.18.1->keras_nlp) (1.17.2)\nRequirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow<2.19,>=2.18.0->tensorflow-text->keras-hub==0.18.1->keras_nlp) (1.73.1)\nRequirement already satisfied: tensorboard<2.19,>=2.18 in /usr/local/lib/python3.11/dist-packages (from tensorflow<2.19,>=2.18.0->tensorflow-text->keras-hub==0.18.1->keras_nlp) (2.18.0)\nRequirement already satisfied: keras>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow<2.19,>=2.18.0->tensorflow-text->keras-hub==0.18.1->keras_nlp) (3.10.0)\nRequirement already satisfied: h5py>=3.11.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow<2.19,>=2.18.0->tensorflow-text->keras-hub==0.18.1->keras_nlp) (3.14.0)\nRequirement already satisfied: ml-dtypes<0.5.0,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow<2.19,>=2.18.0->tensorflow-text->keras-hub==0.18.1->keras_nlp) (0.4.1)\nRequirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow<2.19,>=2.18.0->tensorflow-text->keras-hub==0.18.1->keras_nlp) (0.37.1)\nRequirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->kagglehub->keras-hub==0.18.1->keras_nlp) (3.4.2)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->kagglehub->keras-hub==0.18.1->keras_nlp) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->kagglehub->keras-hub==0.18.1->keras_nlp) (2.5.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->kagglehub->keras-hub==0.18.1->keras_nlp) (2025.6.15)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy->keras-hub==0.18.1->keras_nlp) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy->keras-hub==0.18.1->keras_nlp) (2022.2.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy->keras-hub==0.18.1->keras_nlp) (1.4.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy->keras-hub==0.18.1->keras_nlp) (2024.2.0)\nRequirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from astunparse>=1.6.0->tensorflow<2.19,>=2.18.0->tensorflow-text->keras-hub==0.18.1->keras_nlp) (0.45.1)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy->keras-hub==0.18.1->keras_nlp) (2024.2.0)\nRequirement already satisfied: namex in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow<2.19,>=2.18.0->tensorflow-text->keras-hub==0.18.1->keras_nlp) (0.1.0)\nRequirement already satisfied: optree in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow<2.19,>=2.18.0->tensorflow-text->keras-hub==0.18.1->keras_nlp) (0.16.0)\nRequirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow<2.19,>=2.18.0->tensorflow-text->keras-hub==0.18.1->keras_nlp) (3.8.2)\nRequirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow<2.19,>=2.18.0->tensorflow-text->keras-hub==0.18.1->keras_nlp) (0.7.2)\nRequirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow<2.19,>=2.18.0->tensorflow-text->keras-hub==0.18.1->keras_nlp) (3.1.3)\nRequirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.11/dist-packages (from werkzeug>=1.0.1->tensorboard<2.19,>=2.18->tensorflow<2.19,>=2.18.0->tensorflow-text->keras-hub==0.18.1->keras_nlp) (3.0.2)\nUsing cached keras_hub-0.18.1-py3-none-any.whl (691 kB)\nInstalling collected packages: keras-hub\n  Attempting uninstall: keras-hub\n    Found existing installation: keras-hub 0.21.1\n    Uninstalling keras-hub-0.21.1:\n      Successfully uninstalled keras-hub-0.21.1\nSuccessfully installed keras-hub-0.18.1\nNote: you may need to restart the kernel to use updated packages.\n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"template = \"Instruction:\\n{instruction}\\n\\nResponse:\\n{response}\"\n\nprompt = template.format(\n    instruction=\"What should I do on a trip to Europe?\",\n    response=\"\",\n)\n# sampler = keras_nlp.samplers.TopKSampler(k=5, seed=2)\n# gemma_lm.compile(sampler=sampler)\n\ngemma_lm.compile()\nprint(gemma_lm.generate(prompt, max_length=256))","metadata":{"id":"ZwQz3xxxKciD","outputId":"078a9031-9117-4eff-fc98-48e5c21cbd82","trusted":true,"execution":{"iopub.status.busy":"2025-07-11T03:13:11.420631Z","iopub.execute_input":"2025-07-11T03:13:11.421451Z","iopub.status.idle":"2025-07-11T03:13:32.617587Z","shell.execute_reply.started":"2025-07-11T03:13:11.421418Z","shell.execute_reply":"2025-07-11T03:13:32.616733Z"}},"outputs":[{"name":"stdout","text":"Instruction:\nWhat should I do on a trip to Europe?\n\nResponse:\nEurope is a great place to visit. It is a great place to visit for a variety of reasons. It is a great place to visit for a variety of reasons. It is a great place to visit for a variety of reasons. It is a great place to visit for a variety of reasons. It is a great place to visit for a variety of reasons. It is a great place to visit for a variety of reasons. It is a great place to visit for a variety of reasons. It is a great place to visit for a variety of reasons. It is a great place to visit for a variety of reasons. It is a great place to visit for a variety of reasons. It is a great place to visit for a variety of reasons. It is a great place to visit for a variety of reasons. It is a great place to visit for a variety of reasons. It is a great place to visit for a variety of reasons. It is a great place to visit for a variety of reasons. It is a great place to visit for a variety of reasons. It is a great place to visit for a variety of reasons. It is a great place to visit for a\n","output_type":"stream"}],"execution_count":18},{"cell_type":"markdown","source":"The model responds with generic tips on how to plan a trip.","metadata":{"id":"AePQUIs2h-Ks"}},{"cell_type":"markdown","source":"### Photosynthesis prompt\n\nPrompt the model to explain photosynthesis in terms simple enough for a 5 year old child to understand.","metadata":{"id":"YQ74Zz_S0iVv"}},{"cell_type":"code","source":"prompt = template.format(\n    instruction=\"Explain the process of photosynthesis in a way that a child could understand.\",\n    response=\"\",\n)\nprint(gemma_lm.generate(prompt, max_length=256))","metadata":{"id":"lorJMbsusgoo","outputId":"ece820e0-f1b2-4d34-b6d5-10cf04a53b31","trusted":true,"execution":{"iopub.status.busy":"2025-07-11T03:13:41.735262Z","iopub.execute_input":"2025-07-11T03:13:41.735545Z","iopub.status.idle":"2025-07-11T03:13:44.623820Z","shell.execute_reply.started":"2025-07-11T03:13:41.735525Z","shell.execute_reply":"2025-07-11T03:13:44.623004Z"}},"outputs":[{"name":"stdout","text":"Instruction:\nExplain the process of photosynthesis in a way that a child could understand.\n\nResponse:\nPhotosynthesis is the process by which plants and other organisms use sunlight to convert carbon dioxide and water into oxygen and glucose. The process of photosynthesis is a series of chemical reactions that occur in the chloroplasts of plants and other photosynthetic organisms. The first step in photosynthesis is the absorption of sunlight by the chlorophyll in the chloroplasts. The energy from the sunlight is used to split water molecules into hydrogen and oxygen atoms. The hydrogen atoms are then used to form the sugar glucose, while the oxygen atoms are released into the atmosphere. The oxygen atoms are then used by plants and other organisms to create energy and to carry out cellular respiration.\n","output_type":"stream"}],"execution_count":20},{"cell_type":"markdown","source":"The model response contains words that might not be easy to understand for a child such as chlorophyll.","metadata":{"id":"WBQieduRizZf"}},{"cell_type":"markdown","source":"## LoRA fine-tuning\n\nThis section shows you how to do fine-tuning using the Low Rank Adaptation (LoRA) tuning technique. This approach allows you to change the behavior of Gemma models using fewer compute resources.","metadata":{"id":"Pt7Nr6a7tItO"}},{"cell_type":"markdown","source":"### Load dataset\n\nPrepare a dataset for tuning by downloading an existing data set and formatting if for use with the the Keras `fit()` fine-tuning method. This tutorial uses the [Databricks Dolly 15k dataset](https://huggingface.co/datasets/databricks/databricks-dolly-15k) for fine-tuning. The dataset contains 15,000 high-quality human-generated prompt and response pairs specifically designed for tuning generative models.","metadata":{"id":"9T7xe_jzslv4"}},{"cell_type":"code","source":"!wget -O databricks-dolly-15k.jsonl https://huggingface.co/datasets/databricks/databricks-dolly-15k/resolve/main/databricks-dolly-15k.jsonl","metadata":{"id":"xRaNCPUXKoa7","outputId":"a494a9c3-c872-4952-b19e-d1b79d834659","trusted":true,"execution":{"iopub.status.busy":"2025-07-11T03:13:52.173092Z","iopub.execute_input":"2025-07-11T03:13:52.173603Z","iopub.status.idle":"2025-07-11T03:13:52.633933Z","shell.execute_reply.started":"2025-07-11T03:13:52.173582Z","shell.execute_reply":"2025-07-11T03:13:52.632971Z"}},"outputs":[{"name":"stdout","text":"--2025-07-11 03:13:52--  https://huggingface.co/datasets/databricks/databricks-dolly-15k/resolve/main/databricks-dolly-15k.jsonl\nResolving huggingface.co (huggingface.co)... 3.171.171.6, 3.171.171.104, 3.171.171.65, ...\nConnecting to huggingface.co (huggingface.co)|3.171.171.6|:443... connected.\nHTTP request sent, awaiting response... 302 Found\nLocation: https://cdn-lfs.hf.co/repos/34/ac/34ac588cc580830664f592597bb6d19d61639eca33dc2d6bb0b6d833f7bfd552/2df9083338b4abd6bceb5635764dab5d833b393b55759dffb0959b6fcbf794ec?response-content-disposition=inline%3B+filename*%3DUTF-8%27%27databricks-dolly-15k.jsonl%3B+filename%3D%22databricks-dolly-15k.jsonl%22%3B&Expires=1752207232&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTc1MjIwNzIzMn19LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2RuLWxmcy5oZi5jby9yZXBvcy8zNC9hYy8zNGFjNTg4Y2M1ODA4MzA2NjRmNTkyNTk3YmI2ZDE5ZDYxNjM5ZWNhMzNkYzJkNmJiMGI2ZDgzM2Y3YmZkNTUyLzJkZjkwODMzMzhiNGFiZDZiY2ViNTYzNTc2NGRhYjVkODMzYjM5M2I1NTc1OWRmZmIwOTU5YjZmY2JmNzk0ZWM%7EcmVzcG9uc2UtY29udGVudC1kaXNwb3NpdGlvbj0qIn1dfQ__&Signature=AJaedY3IdxiTBUrPH1MShGT1RkHR8eepwV1APc2k9HTfg8dhxRQTHc2zSGHpTYsP1KlgOenjV7HwrW8qs65UFrUGqa0DS2QpIa7tl2iBv-qlnrYc3kS0-5rtmL-KulQeydJ6bb7ggOEI%7EEjnuqkoAMJwgEP9DulBLh8gXhWezFhhL53Mx1IVkfis5BXXaL1Wt7eidluv61RizKHg9IMzLJ50c4BY6MMEbeiOwtQ0pFM35qwMOFVLZF-Q09ZbFn220m5Bbsk8EnqQQsaT-cKvzR0jEOvw5qQEOlXapexz-sfMFGKrYw0NWhYvGnhSmROQtAX2laGUF8Q1igk1cWv0rg__&Key-Pair-Id=K3RPWS32NSSJCE [following]\n--2025-07-11 03:13:52--  https://cdn-lfs.hf.co/repos/34/ac/34ac588cc580830664f592597bb6d19d61639eca33dc2d6bb0b6d833f7bfd552/2df9083338b4abd6bceb5635764dab5d833b393b55759dffb0959b6fcbf794ec?response-content-disposition=inline%3B+filename*%3DUTF-8%27%27databricks-dolly-15k.jsonl%3B+filename%3D%22databricks-dolly-15k.jsonl%22%3B&Expires=1752207232&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTc1MjIwNzIzMn19LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2RuLWxmcy5oZi5jby9yZXBvcy8zNC9hYy8zNGFjNTg4Y2M1ODA4MzA2NjRmNTkyNTk3YmI2ZDE5ZDYxNjM5ZWNhMzNkYzJkNmJiMGI2ZDgzM2Y3YmZkNTUyLzJkZjkwODMzMzhiNGFiZDZiY2ViNTYzNTc2NGRhYjVkODMzYjM5M2I1NTc1OWRmZmIwOTU5YjZmY2JmNzk0ZWM%7EcmVzcG9uc2UtY29udGVudC1kaXNwb3NpdGlvbj0qIn1dfQ__&Signature=AJaedY3IdxiTBUrPH1MShGT1RkHR8eepwV1APc2k9HTfg8dhxRQTHc2zSGHpTYsP1KlgOenjV7HwrW8qs65UFrUGqa0DS2QpIa7tl2iBv-qlnrYc3kS0-5rtmL-KulQeydJ6bb7ggOEI%7EEjnuqkoAMJwgEP9DulBLh8gXhWezFhhL53Mx1IVkfis5BXXaL1Wt7eidluv61RizKHg9IMzLJ50c4BY6MMEbeiOwtQ0pFM35qwMOFVLZF-Q09ZbFn220m5Bbsk8EnqQQsaT-cKvzR0jEOvw5qQEOlXapexz-sfMFGKrYw0NWhYvGnhSmROQtAX2laGUF8Q1igk1cWv0rg__&Key-Pair-Id=K3RPWS32NSSJCE\nResolving cdn-lfs.hf.co (cdn-lfs.hf.co)... 18.160.78.87, 18.160.78.43, 18.160.78.76, ...\nConnecting to cdn-lfs.hf.co (cdn-lfs.hf.co)|18.160.78.87|:443... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: 13085339 (12M) [text/plain]\nSaving to: ‘databricks-dolly-15k.jsonl’\n\ndatabricks-dolly-15 100%[===================>]  12.48M  --.-KB/s    in 0.1s    \n\n2025-07-11 03:13:52 (129 MB/s) - ‘databricks-dolly-15k.jsonl’ saved [13085339/13085339]\n\n","output_type":"stream"}],"execution_count":21},{"cell_type":"markdown","source":"### Format tuning data\n\nFormat the downloaded data for use with the Keras `fit()` method. The following code extracts a subset of the training examples to execute the notebook faster. Consider using more training data for higher quality fine-tuning.","metadata":{"id":"45UpBDfBgf0I"}},{"cell_type":"code","source":"import json\n\nprompts = []\nresponses = []\nline_count = 0\n\nwith open(\"databricks-dolly-15k.jsonl\") as file:\n    for line in file:\n        if line_count >= 1000:\n            break  # Limit the training examples, to reduce execution time.\n\n        examples = json.loads(line)\n        # Filter out examples with context, to keep it simple.\n        if examples[\"context\"]:\n            continue\n        # Format data into prompts and response lists.\n        prompts.append(examples[\"instruction\"])\n        responses.append(examples[\"response\"])\n\n        line_count += 1\n\ndata = {\n    \"prompts\": prompts,\n    \"responses\": responses\n}","metadata":{"id":"ZiS-KU9osh_N","trusted":true,"execution":{"iopub.status.busy":"2025-07-11T03:14:01.122793Z","iopub.execute_input":"2025-07-11T03:14:01.123455Z","iopub.status.idle":"2025-07-11T03:14:01.146924Z","shell.execute_reply.started":"2025-07-11T03:14:01.123424Z","shell.execute_reply":"2025-07-11T03:14:01.146105Z"}},"outputs":[],"execution_count":22},{"cell_type":"markdown","source":"### Configure LoRA tuning\n\nActivate LoRA tuning using the Keras `model.backbone.enable_lora()` method, including a LoRA rank value. The *LoRA rank* determines the dimensionality of the trainable matrices that are added to the original weights of the LLM. It controls the expressiveness and precision of the fine-tuning adjustments. A higher rank means more detailed changes are possible, but also means more trainable parameters. A lower rank means less computational overhead, but potentially less precise adaptation.\n\nThis example uses a LoRA rank of 4. In practice, begin with a relatively small rank (such as 4, 8, 16). This setting is computationally efficient for experimentation. Train your model with this rank and evaluate the performance improvement on your task. Gradually increase the rank in subsequent trials and see if that further boosts performance.","metadata":{"id":"cBLW5hiGj31i"}},{"cell_type":"code","source":"# Enable LoRA for the model and set the LoRA rank to 4.\ngemma_lm.backbone.enable_lora(rank=4)","metadata":{"id":"RCucu6oHz53G","trusted":true,"execution":{"iopub.status.busy":"2025-07-11T03:14:06.798616Z","iopub.execute_input":"2025-07-11T03:14:06.799440Z","iopub.status.idle":"2025-07-11T03:14:06.842769Z","shell.execute_reply.started":"2025-07-11T03:14:06.799415Z","shell.execute_reply":"2025-07-11T03:14:06.841680Z"}},"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_187/3240510606.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Enable LoRA for the model and set the LoRA rank to 4.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mgemma_lm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackbone\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menable_lora\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrank\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/keras_hub/src/models/backbone.py\u001b[0m in \u001b[0;36menable_lora\u001b[0;34m(self, rank, target_names)\u001b[0m\n\u001b[1;32m    219\u001b[0m             raise ValueError(\n\u001b[1;32m    220\u001b[0m                 \u001b[0;34m\"There are no lora-enabled layers in this model. \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 221\u001b[0;31m                 \u001b[0;34m\"Make sure to call `.enable_lora(rank)` first.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    222\u001b[0m             )\n\u001b[1;32m    223\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mendswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\".lora.h5\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/keras/src/layers/core/einsum_dense.py\u001b[0m in \u001b[0;36menable_lora\u001b[0;34m(self, rank, lora_alpha, a_initializer, b_initializer)\u001b[0m\n\u001b[1;32m    233\u001b[0m             )\n\u001b[1;32m    234\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlora_enabled\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 235\u001b[0;31m             raise ValueError(\n\u001b[0m\u001b[1;32m    236\u001b[0m                 \u001b[0;34m\"lora is already enabled. This can only be done once per layer.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    237\u001b[0m             )\n","\u001b[0;31mValueError\u001b[0m: lora is already enabled. This can only be done once per layer."],"ename":"ValueError","evalue":"lora is already enabled. This can only be done once per layer.","output_type":"error"}],"execution_count":23},{"cell_type":"markdown","source":"Check the model summary after setting the LoRA rank. Notice that enabling LoRA reduces the number of trainable parameters significantly compared to the total number of parameters in the model:","metadata":{"id":"PlMLp_NVbRoQ"}},{"cell_type":"code","source":"gemma_lm.summary()","metadata":{"id":"KqYyS0gm6pNy","trusted":true,"execution":{"iopub.status.busy":"2025-07-11T03:14:14.836598Z","iopub.execute_input":"2025-07-11T03:14:14.837213Z","iopub.status.idle":"2025-07-11T03:14:14.865555Z","shell.execute_reply.started":"2025-07-11T03:14:14.837189Z","shell.execute_reply":"2025-07-11T03:14:14.864880Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"\u001b[1mPreprocessor: \"gemma3_causal_lm_preprocessor\"\u001b[0m\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Preprocessor: \"gemma3_causal_lm_preprocessor\"</span>\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                                                 \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m                                  Config\u001b[0m\u001b[1m \u001b[0m┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n│ gemma3_tokenizer (\u001b[38;5;33mGemma3Tokenizer\u001b[0m)                            │                      Vocab size: \u001b[38;5;34m262,144\u001b[0m │\n└───────────────────────────────────────────────────────────────┴──────────────────────────────────────────┘\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n┃<span style=\"font-weight: bold\"> Layer (type)                                                  </span>┃<span style=\"font-weight: bold\">                                   Config </span>┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n│ gemma3_tokenizer (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Gemma3Tokenizer</span>)                            │                      Vocab size: <span style=\"color: #00af00; text-decoration-color: #00af00\">262,144</span> │\n└───────────────────────────────────────────────────────────────┴──────────────────────────────────────────┘\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1mModel: \"gemma3_causal_lm\"\u001b[0m\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"gemma3_causal_lm\"</span>\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                 \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape             \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mConnected to              \u001b[0m\u001b[1m \u001b[0m┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n│ padding_mask (\u001b[38;5;33mInputLayer\u001b[0m)     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m)              │               \u001b[38;5;34m0\u001b[0m │ -                          │\n├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n│ token_ids (\u001b[38;5;33mInputLayer\u001b[0m)        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m)              │               \u001b[38;5;34m0\u001b[0m │ -                          │\n├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n│ gemma3_backbone               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1152\u001b[0m)        │   \u001b[38;5;34m1,000,538,240\u001b[0m │ padding_mask[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],        │\n│ (\u001b[38;5;33mGemma3Backbone\u001b[0m)              │                           │                 │ token_ids[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]            │\n├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n│ token_embedding               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m262144\u001b[0m)      │     \u001b[38;5;34m301,989,888\u001b[0m │ gemma3_backbone[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]      │\n│ (\u001b[38;5;33mReversibleEmbedding\u001b[0m)         │                           │                 │                            │\n└───────────────────────────────┴───────────────────────────┴─────────────────┴────────────────────────────┘\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n┃<span style=\"font-weight: bold\"> Layer (type)                  </span>┃<span style=\"font-weight: bold\"> Output Shape              </span>┃<span style=\"font-weight: bold\">         Param # </span>┃<span style=\"font-weight: bold\"> Connected to               </span>┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n│ padding_mask (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>)              │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                          │\n├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n│ token_ids (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>)              │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                          │\n├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n│ gemma3_backbone               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1152</span>)        │   <span style=\"color: #00af00; text-decoration-color: #00af00\">1,000,538,240</span> │ padding_mask[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],        │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Gemma3Backbone</span>)              │                           │                 │ token_ids[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]            │\n├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n│ token_embedding               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">262144</span>)      │     <span style=\"color: #00af00; text-decoration-color: #00af00\">301,989,888</span> │ gemma3_backbone[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]      │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">ReversibleEmbedding</span>)         │                           │                 │                            │\n└───────────────────────────────┴───────────────────────────┴─────────────────┴────────────────────────────┘\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Total params: \u001b[0m\u001b[38;5;34m1,000,538,240\u001b[0m (3.73 GB)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,000,538,240</span> (3.73 GB)\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m300,544\u001b[0m (1.15 MB)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">300,544</span> (1.15 MB)\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m1,000,237,696\u001b[0m (3.73 GB)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,000,237,696</span> (3.73 GB)\n</pre>\n"},"metadata":{}}],"execution_count":24},{"cell_type":"markdown","source":"Configure the rest of the fine-tuning settings, including the preprocessor settings, optimizer, number of tuning epochs, and batch size:","metadata":{"id":"hQQ47kcdpbZ9"}},{"cell_type":"code","source":"# Limit the input sequence length to 256 (to control memory usage).\ngemma_lm.preprocessor.sequence_length = 256\n# Use AdamW (a common optimizer for transformer models).\noptimizer = keras.optimizers.AdamW(\n    learning_rate=5e-5,\n    weight_decay=0.01,\n)\n# Exclude layernorm and bias terms from decay.\noptimizer.exclude_from_weight_decay(var_names=[\"bias\", \"scale\"])\n\ngemma_lm.compile(\n    loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n    optimizer=optimizer,\n    weighted_metrics=[keras.metrics.SparseCategoricalAccuracy()],\n)","metadata":{"id":"p9sBNH8SAjgB","trusted":true,"execution":{"iopub.status.busy":"2025-07-11T03:14:37.001022Z","iopub.execute_input":"2025-07-11T03:14:37.001348Z","iopub.status.idle":"2025-07-11T03:14:37.189801Z","shell.execute_reply.started":"2025-07-11T03:14:37.001321Z","shell.execute_reply":"2025-07-11T03:14:37.189174Z"}},"outputs":[],"execution_count":25},{"cell_type":"markdown","source":"### Run the fine-tune process\n\nRun the fine-tuning process using the `fit()` method. This process can take several minutes depending on your compute resources, data size, and number of epochs:","metadata":{"id":"OA0ozGC66tk1"}},{"cell_type":"code","source":"gemma_lm.fit(data, epochs=1, batch_size=1)","metadata":{"id":"_Peq7TnLtHse","outputId":"4b0b4ffa-1ab9-45cc-f8bd-81079d9f1a29","trusted":true,"execution":{"iopub.status.busy":"2025-07-11T03:14:39.783065Z","iopub.execute_input":"2025-07-11T03:14:39.783587Z","iopub.status.idle":"2025-07-11T03:20:39.080049Z","shell.execute_reply.started":"2025-07-11T03:14:39.783561Z","shell.execute_reply":"2025-07-11T03:20:39.079421Z"}},"outputs":[{"name":"stdout","text":"\u001b[1m1000/1000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m356s\u001b[0m 339ms/step - loss: 0.6256 - sparse_categorical_accuracy: 0.5240\n","output_type":"stream"},{"execution_count":26,"output_type":"execute_result","data":{"text/plain":"<keras.src.callbacks.history.History at 0x7f06a8204b90>"},"metadata":{}}],"execution_count":26},{"cell_type":"markdown","source":"#### Mixed precision fine-tuning on NVIDIA GPUs\n\nFull precision is recommended for fine-tuning. When fine-tuning on NVIDIA GPUs, you can use mixed precision (`keras.mixed_precision.set_global_policy('mixed_bfloat16')`) to speed up training with minimal effect on training quality.","metadata":{"id":"bx3m8f1dB7nk"}},{"cell_type":"code","source":"# Uncomment the line below if you want to enable mixed precision training on GPUs\n# keras.mixed_precision.set_global_policy('mixed_bfloat16')","metadata":{"id":"T0lHxEDX03gp","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Inference after fine-tuning\n\nAfter fine-tuning, you should see changes in the responses when the tuned model is given the same prompt.","metadata":{"id":"4yd-1cNw1dTn"}},{"cell_type":"markdown","source":"### Europe trip prompt\n\nTry the Europe trip prompt from earlier and note the differences in the response.","metadata":{"id":"H55JYJ1a1Kos"}},{"cell_type":"code","source":"prompt = template.format(\n    instruction=\"What should I do on a trip to Europe?\",\n    response=\"\",\n)\n# sampler = keras_nlp.samplers.TopKSampler(k=5, seed=2)\n# gemma_lm.compile(sampler=sampler)\ngemma_lm.compile()\nprint(gemma_lm.generate(prompt, max_length=256))","metadata":{"id":"Y7cDJHy8WfCB","outputId":"150b00a3-2545-4261-b9d6-c5db663bde23","trusted":true,"execution":{"iopub.status.busy":"2025-07-11T03:22:25.870291Z","iopub.execute_input":"2025-07-11T03:22:25.870934Z","iopub.status.idle":"2025-07-11T03:22:47.163329Z","shell.execute_reply.started":"2025-07-11T03:22:25.870909Z","shell.execute_reply":"2025-07-11T03:22:47.162670Z"}},"outputs":[{"name":"stdout","text":"Instruction:\nWhat should I do on a trip to Europe?\n\nResponse:\nEurope is a great place to visit. It is a great place to visit for a variety of reasons. It is a great place to visit for a variety of reasons. It is a great place to visit for a variety of reasons. It is a great place to visit for a variety of reasons. It is a great place to visit for a variety of reasons. It is a great place to visit for a variety of reasons. It is a great place to visit for a variety of reasons. It is a great place to visit for a variety of reasons. It is a great place to visit for a variety of reasons. It is a great place to visit for a variety of reasons. It is a great place to visit for a variety of reasons. It is a great place to visit for a variety of reasons. It is a great place to visit for a variety of reasons. It is a great place to visit for a variety of reasons. It is a great place to visit for a variety of reasons. It is a great place to visit for a variety of reasons. It is a great place to visit for a variety of reasons. It is a great place to visit for a\n","output_type":"stream"}],"execution_count":28},{"cell_type":"markdown","source":"The model now provides a shorter response to a question about visiting Europe.","metadata":{"id":"OXP6gg2mjs6u"}},{"cell_type":"markdown","source":"### Photosynthesis prompt\n\nTry the photosynthesis explanation prompt from earlier and note the differences in the response.","metadata":{"id":"H7nVd8Mi1Yta"}},{"cell_type":"code","source":"prompt = template.format(\n    instruction=\"Explain the process of photosynthesis in a way that a child could understand.\",\n    response=\"\",\n)\nprint(gemma_lm.generate(prompt, max_length=256))","metadata":{"id":"X-2sYl2jqwl7","outputId":"50ddf6e3-8f92-4c72-d9cb-131f548bcbbd","trusted":true,"execution":{"iopub.status.busy":"2025-07-11T03:22:47.164302Z","iopub.execute_input":"2025-07-11T03:22:47.164576Z","iopub.status.idle":"2025-07-11T03:22:49.963830Z","shell.execute_reply.started":"2025-07-11T03:22:47.164549Z","shell.execute_reply":"2025-07-11T03:22:49.963163Z"}},"outputs":[{"name":"stdout","text":"Instruction:\nExplain the process of photosynthesis in a way that a child could understand.\n\nResponse:\nPhotosynthesis is the process by which plants and other organisms use sunlight to convert carbon dioxide and water into oxygen and glucose. The process of photosynthesis is a series of chemical reactions that occur in the chloroplasts of plants and other photosynthetic organisms. The first step in photosynthesis is the absorption of sunlight by the chlorophyll in the chloroplasts. The energy from the sunlight is used to split water molecules into hydrogen and oxygen atoms. The hydrogen atoms are then used to form the sugar glucose, while the oxygen atoms are released into the atmosphere. The oxygen atoms are then used by plants and other organisms to create energy and to carry out cellular respiration.\n","output_type":"stream"}],"execution_count":29},{"cell_type":"markdown","source":"The model now explains photosynthesis in simpler terms.","metadata":{"id":"PCmAmqrvkEhc"}},{"cell_type":"markdown","source":"## Improving fine-tune results\n\nFor demonstration purposes, this tutorial fine-tunes the model on a small subset of the dataset for just one epoch and with a low LoRA rank value. To get better responses from the fine-tuned model, you can experiment with:\n\n1. Increasing the size of the fine-tuning dataset\n2. Training for more steps (epochs)\n3. Setting a higher LoRA rank\n4. Modifying the hyperparameter values such as `learning_rate` and `weight_decay`.","metadata":{"id":"I8kFG12l0mVe"}},{"cell_type":"markdown","source":"## Summary and next steps\n\nThis tutorial covered LoRA fine-tuning on a Gemma model using Keras. Check out the following docs next:\n\n* Learn how to [generate text with a Gemma model](https://ai.google.dev/gemma/docs/get_started).\n* Learn how to perform [distributed fine-tuning and inference on a Gemma model](https://ai.google.dev/gemma/docs/core/distributed_tuning).\n* Learn how to [use Gemma open models with Vertex AI](https://cloud.google.com/vertex-ai/docs/generative-ai/open-models/use-gemma).\n* Learn how to [fine-tune Gemma using Keras and deploy to Vertex AI](https://github.com/GoogleCloudPlatform/vertex-ai-samples/blob/main/notebooks/community/model_garden/model_garden_gemma_kerasnlp_to_vertexai.ipynb).","metadata":{"id":"gSsRdeiof_rJ"}}]}