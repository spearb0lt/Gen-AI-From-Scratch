{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":64148,"databundleVersionId":7669720,"sourceType":"competition"},{"sourceId":7970419,"sourceType":"datasetVersion","datasetId":4616621},{"sourceId":28785,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":8318}],"dockerImageVersionId":30683,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Introduction\n## Gemma LoRA Fine-tuning for Beginners with Hugging Face\n\nIn this notebook, we'll learn the very basics of using the Gemma model, incorporating the powerful tools from Hugging Face. It's focused on the simplest content without any complex processing. This practical exercise is about training a Large Language Model (LLM) to generate Python Q&A using the Gemma model with the support of Hugging Face libraries.\n\n### Table of Contents:\n \n1. What is Gemma?<br>\n2. Package Installation and Importing<br>\n3. Data Loading <br>\n4. Data Preprocessing for Training<br>\n5. Loading the Gemma Model<br>\n7. Q & A Results Before Finetuning<br>\n7. Applying Gemma LoRA<br>\n8. Training Gemma<br>\n9. Q & A Results After Finetuning<br>\n10. Conclusion<br>\n\n### Dataset Used\n- [Dataset_Python_Question_Answer](https://www.kaggle.com/datasets/chinmayadatt/dataset-python-question-answer) : This dataset is about Python programming. Question and answers are generated using Gemma. There are more than four hundred questions and their corresponding answers about Python programming.\n\n---","metadata":{}},{"cell_type":"markdown","source":"# 1.What is Gemma?\n\nGemma is a powerful machine learning model designed for a wide range of tasks. This section will introduce the basics of Gemma, its use cases, and why it's beneficial for your projects.\n\n### Summary\n\n- Gemma models are built from the ground up to be lightweight and state-of-the-art. They are text-to-text, decoder-only large language models, available primarily in English.\n- They come with open weights, offering both pre-trained and instruction-tuned variants to suit a wide array of text generation tasks.\n- Ideal for applications such as question answering, summarization, and reasoning, Gemma models can be deployed on relatively modest hardware, including laptops and desktops, or within your own cloud infrastructure.\n\n### Description\n\n- **Lightweight and Open**: Gemma models are designed to be both powerful and accessible, embodying Google's commitment to democratizing state-of-the-art AI technology.\n- **Versatile Applications**: Whether it's generating answers to questions, summarizing documents, or facilitating complex reasoning tasks, Gemma models are equipped to handle a diverse set of challenges.\n- **Democratizing AI**: By making Gemma models lightweight and open, Google ensures that cutting-edge AI technology is no longer confined to those with access to extensive computational resources.\n\n### Inputs and Outputs\n\n- **Input**: Gemma models take in text strings, which can range from questions and prompts to longer documents that require summarization.\n- **Output**: In response, they generate text in English, offering answers, summaries, or other forms of text-based output, tailored to the input provided.\n","metadata":{}},{"cell_type":"markdown","source":"# 2. Package Installation and Importing\n\nBefore we start, it's essential to install all necessary packages, including Gemma itself. This part will cover the installation process step by step.","metadata":{}},{"cell_type":"code","source":"# Install specific versions of PEFT, evaluate, transformers, accelerate, and bitsandbytes packages quietly without showing output.\n!pip install -q -U peft evaluate transformers==4.38.0 accelerate==0.27.2 bitsandbytes==0.42.0 peft==0.8.2\n\n# Upgrade and quietly install the latest versions of the trl and datasets packages.\n!pip install -U -q trl==0.7.11 datasets==2.17.1","metadata":{"execution":{"iopub.status.busy":"2024-04-13T17:18:21.703816Z","iopub.execute_input":"2024-04-13T17:18:21.704195Z","iopub.status.idle":"2024-04-13T17:18:48.412834Z","shell.execute_reply.started":"2024-04-13T17:18:21.704164Z","shell.execute_reply":"2024-04-13T17:18:48.411516Z"},"trusted":true},"execution_count":28,"outputs":[{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"}]},{"cell_type":"markdown","source":"### Package Description\n\n#### python basic module\n- `os`: Provides ways to interact with the operating system and its environment variables.\n- `torch`: PyTorch library for deep learning applications.\n- `numpy`: Essential library for linear algebra and mathematical operations.\n- `pandas`: Powerful data processing tool, ideal for handling CSV files and other forms of structured data.\n\n#### transformers module\n- `AutoTokenizer`: Used to automatically load a pre-trained tokenizer.\n- `AutoModelForCausalLM`: Used to automatically load pre-trained models for causal language modeling.\n- `BitsAndBytesConfig`: Configuration class for setting up the Bits and Bytes tokenizer.\n- `AutoConfig`: Used to automatically load the model's configuration.\n- `TrainingArguments`: Defines arguments for training setup.\n\n#### datasets module\n- `Dataset`: A class for handling datasets.\n\n#### peft module\n- `LoraConfig`: A configuration class for configuring the Lora model.\n- `PeftModel`: A class that defines the PEFT model.\n- `prepare_model_for_kbit_training`: A function that prepares a model for k-bit training.\n- `get_peft_model`: Function to get the PEFT model.\n\n#### trl module\n- `SFTTrainer`: Trainer class for SFT (Supervised Fine-Tuning) training.\n\n#### IPython.display module\n- `Markdown`: Used to output text in Markdown format.\n- `display`: Used to display objects in Jupyter notebooks.","metadata":{}},{"cell_type":"code","source":"import os\nimport torch\n\nimport numpy as np\nimport pandas as pd\n\nfrom transformers import (AutoTokenizer, \n                          AutoModelForCausalLM, \n                          BitsAndBytesConfig, \n                          AutoConfig,\n                          TrainingArguments)\n\nfrom datasets import Dataset\nfrom peft import LoraConfig, PeftModel, prepare_model_for_kbit_training, get_peft_model\nfrom trl import SFTTrainer\nfrom IPython.display import Markdown, display","metadata":{"execution":{"iopub.status.busy":"2024-04-13T17:18:48.415542Z","iopub.execute_input":"2024-04-13T17:18:48.416472Z","iopub.status.idle":"2024-04-13T17:18:48.423236Z","shell.execute_reply.started":"2024-04-13T17:18:48.416430Z","shell.execute_reply":"2024-04-13T17:18:48.422147Z"},"trusted":true},"execution_count":29,"outputs":[]},{"cell_type":"code","source":"# Disable CA bundle check. Useful in certain environments where you may encounter SSL errors.\nos.environ['CURL_CA_BUNDLE'] = ''\n\n# Set the order of devices as seen by CUDA to PCI bus ID order. This is to ensure consistency in device selection.\nos.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\n\n# Check if CUDA is available, and if so, specify which GPU(s) to be made visible to the process.\nif torch.cuda.is_available():\n    os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"  # Only set this if CUDA is available.\n    print(\"CUDA is available\")\nelse:\n    print(\"CUDA is not available\")","metadata":{"execution":{"iopub.status.busy":"2024-04-13T17:18:48.424509Z","iopub.execute_input":"2024-04-13T17:18:48.424852Z","iopub.status.idle":"2024-04-13T17:18:48.435321Z","shell.execute_reply.started":"2024-04-13T17:18:48.424820Z","shell.execute_reply":"2024-04-13T17:18:48.434352Z"},"trusted":true},"execution_count":30,"outputs":[{"name":"stdout","text":"CUDA is available\n","output_type":"stream"}]},{"cell_type":"markdown","source":"A tool for tracking and visualizing Machine Learning experiments. Wandb helps you easily manage metrics, hyperparameters, experiment code, and model artifacts during model training.<br>\n<a href=\"https://github.com/wandb/wandb\">wandb github</a>","metadata":{}},{"cell_type":"code","source":"# Wandb for experiment tracking\nimport wandb\n\n# Initialize Weights & Biases (wandb) for experiment tracking.\n# If a wandb account exists, it can typically be used by specifying project and entity.\n# However, for this example, we're disabling wandb to ignore it by setting mode to \"disabled\".\nwandb.init(mode=\"disabled\")","metadata":{"execution":{"iopub.status.busy":"2024-04-13T17:18:48.437549Z","iopub.execute_input":"2024-04-13T17:18:48.437828Z","iopub.status.idle":"2024-04-13T17:18:48.468149Z","shell.execute_reply.started":"2024-04-13T17:18:48.437804Z","shell.execute_reply":"2024-04-13T17:18:48.467256Z"},"trusted":true},"execution_count":31,"outputs":[{"execution_count":31,"output_type":"execute_result","data":{"text/plain":""},"metadata":{}}]},{"cell_type":"markdown","source":"# 3. Data Loading\n\nLoading your data is the first step in the machine learning pipeline. This section will guide you through loading your dataset into the Jupyter notebook environment.\n\n### To download a dataset, follow these simple steps:\n1. Look for the \"Input\" option located below the \"Notebook\" section in the right-side menu.\n2. Click on the \"+ Add Input\" button.\n3. In the search bar that appears, type \"dataset-python-question-answer\".\n4. Find the dataset in the search results and click the \"+\" button to add it to your notebook. This action will automatically download the dataset for you.","metadata":{}},{"cell_type":"code","source":"# The necessary packages `os` and `pandas` are required for this section of the code. \n# However, they have already been imported in the \"2. Package Installation\" section, so their import statements are omitted here to avoid redundancy.\n\n# Define the filename of the target dataset.\n# Natural Language to Python Code\n\ntarget_filename = 'Dataset_Python_Question_Answer.csv'\n\n# Initialize a variable to hold the full path to the target CSV file.\ncsv_file_path = None\n\n# Walk through the directory structure starting from '/kaggle/input'.\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    # Check if the target filename is present in the current directory's list of files.\n    if target_filename in filenames:\n        # Construct the full path to the target file and update the csv_file_path variable.\n        csv_file_path = os.path.join(dirname, target_filename)\n        break  # Exit the loop after finding the target file.\n        \n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session\n\n# Check if the specific CSV file's path has been found.\nif csv_file_path:\n    print(csv_file_path)\nelse:\n    # Print an error message if the specific CSV file was not found.\n    # Also, suggest checking the 'Input' menu to ensure the file has been properly added.\n    print(f\"The specified file '{target_filename}' was not found. Please ensure the file has been correctly added to the 'Input' menu on the right.\")","metadata":{"execution":{"iopub.status.busy":"2024-04-13T17:18:48.469452Z","iopub.execute_input":"2024-04-13T17:18:48.469811Z","iopub.status.idle":"2024-04-13T17:18:48.482294Z","shell.execute_reply.started":"2024-04-13T17:18:48.469769Z","shell.execute_reply":"2024-04-13T17:18:48.481337Z"},"trusted":true},"execution_count":32,"outputs":[{"name":"stdout","text":"/kaggle/input/dataset-python-question-answer/Dataset_Python_Question_Answer.csv\n","output_type":"stream"}]},{"cell_type":"code","source":"# Load the dataset from the identified CSV file.\n# csv_file_path = \"/kaggle/input/dataset-python-question-answer/Dataset_Python_Question_Answer.csv\"\noriginal_data = pd.read_csv(csv_file_path)\n\n# Print the shape of the dataset to understand its dimensions (number of rows and columns).\nprint('original_data shape:',original_data.shape)","metadata":{"execution":{"iopub.status.busy":"2024-04-13T17:18:48.483477Z","iopub.execute_input":"2024-04-13T17:18:48.483737Z","iopub.status.idle":"2024-04-13T17:18:48.507680Z","shell.execute_reply.started":"2024-04-13T17:18:48.483714Z","shell.execute_reply":"2024-04-13T17:18:48.506729Z"},"trusted":true},"execution_count":33,"outputs":[{"name":"stdout","text":"original_data shape: (419, 2)\n","output_type":"stream"}]},{"cell_type":"code","source":"# Display a random sample of 2 rows from the original_data to get a quick overview of the data.\noriginal_data.sample(2)","metadata":{"execution":{"iopub.status.busy":"2024-04-13T17:18:48.508766Z","iopub.execute_input":"2024-04-13T17:18:48.509047Z","iopub.status.idle":"2024-04-13T17:18:48.519507Z","shell.execute_reply.started":"2024-04-13T17:18:48.509022Z","shell.execute_reply":"2024-04-13T17:18:48.518592Z"},"trusted":true},"execution_count":34,"outputs":[{"execution_count":34,"output_type":"execute_result","data":{"text/plain":"                                              Question  \\\n203   What is recursion with a default case in Pyth...   \n278   What is the difference between a dictionary a...   \n\n                                                Answer  \n203  [\"Sure. Here's an example of recursion with a ...  \n278  [\"Sure. Here's the difference between a dictio...  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Question</th>\n      <th>Answer</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>203</th>\n      <td>What is recursion with a default case in Pyth...</td>\n      <td>[\"Sure. Here's an example of recursion with a ...</td>\n    </tr>\n    <tr>\n      <th>278</th>\n      <td>What is the difference between a dictionary a...</td>\n      <td>[\"Sure. Here's the difference between a dictio...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"markdown","source":"# 4. Data Preprocessing for Training\n\nBefore initiating the training process with Google's Gemma, a pivotal step involves the preparation of our dataset. The core of this stage is to align our dataset with the specifications required by Gemma, ensuring optimal compatibility and efficiency in training. The process commences with the strategic manipulation of our dataset, specifically focusing on the 'Question' and 'Answer' columns. These columns are instrumental as we meticulously combine them to form comprehensive training examples, thereby facilitating a seamless training experience.\n\nA critical aspect to acknowledge during data preprocessing is the management of data length. Given that the Gemma model operates as a Large Language Model (LLM), it's imperative to assess the length of our training data. Training with excessively lengthy data could impose substantial demands on GPU resources, potentially hindering the efficiency of the process. To circumvent this challenge and optimize resource utilization, we advocate for the exclusion of unduly long data from the training set. This strategic decision not only preserves GPU resources but also ensures a more streamlined and effective training workflow.","metadata":{}},{"cell_type":"code","source":"question_column = \"Question\"\nanswer_column = \"Answer\"\n\n# Calculate the length of each 'Question' and 'Answer' combined and add it as a new column\noriginal_data['text_length'] = original_data[question_column].str.len() + original_data[answer_column].str.len()\n\n# Calculate the average length of 'Answer' in the filtered dataset\naverage_length = int(original_data['text_length'].mean())\n\n# Find the shortest and longest lengths of 'Answer' in the filtered dataset\nshortest_length = int(original_data['text_length'].min())\nlongest_length = int(original_data['text_length'].max())\n\n# Print the statistics\nprint(\"Average length of 'Question and Answer' in original dataset:\", average_length)\nprint(\"Shortest length of 'Question and Answer' in original dataset:\", shortest_length)\nprint(\"Longest length of 'Question and Answer' in original dataset:\", longest_length)","metadata":{"execution":{"iopub.status.busy":"2024-04-13T17:18:48.521215Z","iopub.execute_input":"2024-04-13T17:18:48.521560Z","iopub.status.idle":"2024-04-13T17:18:48.535154Z","shell.execute_reply.started":"2024-04-13T17:18:48.521525Z","shell.execute_reply":"2024-04-13T17:18:48.534281Z"},"trusted":true},"execution_count":35,"outputs":[{"name":"stdout","text":"Average length of 'Question and Answer' in original dataset: 1708\nShortest length of 'Question and Answer' in original dataset: 139\nLongest length of 'Question and Answer' in original dataset: 3511\n","output_type":"stream"}]},{"cell_type":"code","source":"# Calculate the median length of 'text_length' to set a threshold for filtering\nmedian_text_length_threshold = int(original_data['text_length'].quantile(0.5))\n\n# Retain only rows where 'text_length' is less than or equal to the median text length\nfiltered_data = original_data[original_data['text_length'] <= median_text_length_threshold]\n\n# Output the number of entries before and after filtering to assess the impact\nprint(\"Number of entries before filtering:\", len(original_data))\nprint(\"Number of entries after filtering:\", len(filtered_data))\n\nprint(\"---\"*10)\n\n# Determine the maximum 'text_length' in the filtered dataset\nmax_text_length_in_filtered_data = int(filtered_data['text_length'].max())\n\n# Compare the maximum 'text_length' before and after filtering\nprint(f\"Maximum text length before filtering: {longest_length}\\nMaximum text length after filtering: {max_text_length_in_filtered_data}\")\n","metadata":{"execution":{"iopub.status.busy":"2024-04-13T17:18:48.536471Z","iopub.execute_input":"2024-04-13T17:18:48.536760Z","iopub.status.idle":"2024-04-13T17:18:48.548774Z","shell.execute_reply.started":"2024-04-13T17:18:48.536737Z","shell.execute_reply":"2024-04-13T17:18:48.547920Z"},"trusted":true},"execution_count":36,"outputs":[{"name":"stdout","text":"Number of entries before filtering: 419\nNumber of entries after filtering: 210\n------------------------------\nMaximum text length before filtering: 3511\nMaximum text length after filtering: 1754\n","output_type":"stream"}]},{"cell_type":"code","source":"# Display a random sample of 2 rows from the filtered_data to get a quick overview of the data.\nfiltered_data.sample(2)","metadata":{"execution":{"iopub.status.busy":"2024-04-13T17:18:48.552092Z","iopub.execute_input":"2024-04-13T17:18:48.552445Z","iopub.status.idle":"2024-04-13T17:18:48.563742Z","shell.execute_reply.started":"2024-04-13T17:18:48.552420Z","shell.execute_reply":"2024-04-13T17:18:48.562819Z"},"trusted":true},"execution_count":37,"outputs":[{"execution_count":37,"output_type":"execute_result","data":{"text/plain":"                                              Question  \\\n233   Define a function that takes a list of string...   \n256   Define a function that takes a string and two...   \n\n                                                Answer  text_length  \n233  ['```python', 'def remove_first_letter(strings...         1520  \n256  ['```python', 'def get_first_letter(text, num1...          537  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Question</th>\n      <th>Answer</th>\n      <th>text_length</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>233</th>\n      <td>Define a function that takes a list of string...</td>\n      <td>['```python', 'def remove_first_letter(strings...</td>\n      <td>1520</td>\n    </tr>\n    <tr>\n      <th>256</th>\n      <td>Define a function that takes a string and two...</td>\n      <td>['```python', 'def get_first_letter(text, num1...</td>\n      <td>537</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"markdown","source":"Furthermore, it's **essential** to highlight the integration with the Hugging Face's transformers library, a pivotal component in our data preprocessing journey. This integration necessitates the conversion of our dataset into a specific format, namely `from datasets import Dataset`. This adjustment is crucial as it aligns with the library's requirements, enabling us to leverage its full potential in facilitating the training of the Gemma model. By adhering to this format, we ensure a harmonious and efficient interaction with the transformers library, further enhancing the overall training process.\n<a href=\"https://huggingface.co/docs/transformers/index\">Transformers documentation</a>\n","metadata":{}},{"cell_type":"code","source":"import random\n\n# Convert dataset to Dataset object\ndataset = Dataset.from_pandas(filtered_data)\n\n# Print the entire dataset\nprint(\"<Data structure>\")\nprint(dataset)\n\n# Generate a random index based on the dataset length\nrandom_index = random.randint(0, len(dataset) - 1)\n\n# Print a random sample of the dataset\nprint(\"\\n\\n<Random sample dataset>\")\nprint(\"\\n- Question:\", dataset[random_index][question_column])\nprint(\"\\n- Answer:\", dataset[random_index][answer_column])","metadata":{"execution":{"iopub.status.busy":"2024-04-13T17:18:48.565048Z","iopub.execute_input":"2024-04-13T17:18:48.565449Z","iopub.status.idle":"2024-04-13T17:18:48.586725Z","shell.execute_reply.started":"2024-04-13T17:18:48.565418Z","shell.execute_reply":"2024-04-13T17:18:48.585559Z"},"trusted":true},"execution_count":38,"outputs":[{"name":"stdout","text":"<Data structure>\nDataset({\n    features: ['Question', 'Answer', 'text_length', '__index_level_0__'],\n    num_rows: 210\n})\n\n\n<Random sample dataset>\n\n- Question:  What is the difference between a tuple and a list in Python?\n\n- Answer: [\"Sure, here's the difference between a tuple and a list in Python.\", '**Tuples** are unordered collections of elements that are defined at the time of creation. They are created using parentheses, and the elements are separated by commas, with each element on a separate line. Tuples are immutable, meaning their contents cannot be changed after they are created.', '**Lists** are ordered collections of elements that are defined at the time of creation. They are created using square brackets, and the elements are separated by commas, with each element on a separate line. Lists are mutable, meaning their contents can be changed after they are created.', \"Here's an example to illustrate the difference between tuples and lists:\", '```python', 'tuple1 = (1, 2, 3)', 'list1 = [1, 2, 3]', 'print(tuple1)  # Output: (1, 2, 3)', 'print(list1)  # Output: [1, 2, 3]', '```', 'Tuples are often used when you need to store elements that will not be changed after they are created. Lists are often used when you need to store elements in order, or when you need to be able to modify the contents of the list later on.']\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# 5. Loading the Gemma Model\n\nHere, we'll cover how to load the Gemma model so it's ready for finetuning. This includes where to download the model from and how to load it into your notebook.","metadata":{}},{"cell_type":"markdown","source":"### Adding the Gemma Model\n1. Still in the \"Input\" section of the right-side menu in your Kaggle notebook, click on the \"+ Add Input\" button again.\n2. Below the search bar that appears, click on the \"Models\" option.\n3. In the search bar, type \"Gemma\" to find the model.\n4. From the filtered results, select the Gemma model by clicking on the \"+\" button next to it. Make sure to choose the correct version by noting the framework as \"Transformers\", the variation as \"2b-it\", and the version as \"v3\".\n5. After selecting the correct Gemma model, click on \"Add Model\" at the bottom.\n6. The Gemma model, specifically \"Gemma.v3\", should now be listed under the \"Models\" subsection of the \"Input\" section in the right-side menu of your notebook, indicating successful addition.","metadata":{}},{"cell_type":"code","source":"# Check if gemma/transformers/2b-it/3 exists.\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"execution":{"iopub.status.busy":"2024-04-13T17:18:48.587774Z","iopub.execute_input":"2024-04-13T17:18:48.588063Z","iopub.status.idle":"2024-04-13T17:18:48.596986Z","shell.execute_reply.started":"2024-04-13T17:18:48.588039Z","shell.execute_reply":"2024-04-13T17:18:48.596167Z"},"trusted":true},"execution_count":39,"outputs":[{"name":"stdout","text":"/kaggle/input/data-assistants-with-gemma/submission_categories.txt\n/kaggle/input/data-assistants-with-gemma/submission_instructions.txt\n/kaggle/input/dataset-python-question-answer/Dataset_Python_Question_Answer.csv\n/kaggle/input/gemma/transformers/2b-it/3/model.safetensors.index.json\n/kaggle/input/gemma/transformers/2b-it/3/gemma-2b-it.gguf\n/kaggle/input/gemma/transformers/2b-it/3/config.json\n/kaggle/input/gemma/transformers/2b-it/3/model-00001-of-00002.safetensors\n/kaggle/input/gemma/transformers/2b-it/3/model-00002-of-00002.safetensors\n/kaggle/input/gemma/transformers/2b-it/3/tokenizer.json\n/kaggle/input/gemma/transformers/2b-it/3/tokenizer_config.json\n/kaggle/input/gemma/transformers/2b-it/3/special_tokens_map.json\n/kaggle/input/gemma/transformers/2b-it/3/.gitattributes\n/kaggle/input/gemma/transformers/2b-it/3/tokenizer.model\n/kaggle/input/gemma/transformers/2b-it/3/generation_config.json\n","output_type":"stream"}]},{"cell_type":"markdown","source":"### BitsAndBytesConfig Overview\n\n`BitsAndBytesConfig` is a configuration class provided by the `transformers` library, which is designed for controlling the behavior of model quantization and optimization during both the training and inference phases of model deployment. Quantization is a technique used to reduce the memory footprint and computational requirements of deep learning models by representing model weights and activations in lower-precision data types, such as 8-bit integers (`int8`) or even 4-bit representations.\n\n#### Benefits of Quantization\n\nThe primary benefits of quantization include:\n\n- **Reduced Memory Usage**: Lower-precision representations require less memory, enabling the deployment of larger models on devices with limited memory capacity.\n- **Increased Inference Speed**: Operations with lower-precision data types can be executed faster, thus speeding up the inference time.\n- **Energy Efficiency**: Reduced computational requirements translate to lower energy consumption, which is crucial for mobile and embedded devices.\n\n#### `BitsAndBytesConfig` Parameters\n\nIn the context of the `transformers` library, `BitsAndBytesConfig` allows users to configure the quantization behavior specifically for using the `bitsandbytes` backend. Below is an example configuration along with comments explaining each parameter:\n","metadata":{}},{"cell_type":"code","source":"# Checking for the available device (CPU or GPU)\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n# Available devices print\nprint(\"device:\",device)\n\n# Defining the path to the pre-trained model\nmodel_path = \"/kaggle/input/gemma/transformers/2b-it/3\"\n\n# Loading the tokenizer\ntokenizer = AutoTokenizer.from_pretrained(model_path)\n\n# Defining BitsAndBytesConfig\nbnbConfig = BitsAndBytesConfig(\n    load_in_4bit=True, # Enable loading of the model in 4-bit quantized format.\n    bnb_4bit_quant_type=\"nf4\", # Specify the quantization type. \"nf4\" refers to a specific 4-bit quantization scheme.\n    bnb_4bit_compute_dtype=torch.bfloat16, # Define the data type for computations. bfloat16 offers a good balance between precision and speed.\n)\n\n# Loading the model for causal language modeling\nmodel = AutoModelForCausalLM.from_pretrained(model_path,\n                                             device_map=\"auto\",\n                                             quantization_config=bnbConfig\n                                            )\n\n# Move the model to the specified computing device (CPU or GPU).\n# model = model.to(device)","metadata":{"execution":{"iopub.status.busy":"2024-04-13T17:18:48.598282Z","iopub.execute_input":"2024-04-13T17:18:48.599133Z","iopub.status.idle":"2024-04-13T17:18:55.870752Z","shell.execute_reply.started":"2024-04-13T17:18:48.599089Z","shell.execute_reply":"2024-04-13T17:18:55.869804Z"},"trusted":true},"execution_count":40,"outputs":[{"name":"stdout","text":"device: cuda\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"621dca0d22144e2b8eed798993dfc2a7"}},"metadata":{}}]},{"cell_type":"code","source":"# Print a summary of the model to understand its architecture and the number of parameters.\nmodel","metadata":{"execution":{"iopub.status.busy":"2024-04-13T17:18:55.872024Z","iopub.execute_input":"2024-04-13T17:18:55.872415Z","iopub.status.idle":"2024-04-13T17:18:55.881031Z","shell.execute_reply.started":"2024-04-13T17:18:55.872382Z","shell.execute_reply":"2024-04-13T17:18:55.880177Z"},"trusted":true},"execution_count":41,"outputs":[{"execution_count":41,"output_type":"execute_result","data":{"text/plain":"GemmaForCausalLM(\n  (model): GemmaModel(\n    (embed_tokens): Embedding(256000, 2048, padding_idx=0)\n    (layers): ModuleList(\n      (0-17): 18 x GemmaDecoderLayer(\n        (self_attn): GemmaSdpaAttention(\n          (q_proj): Linear4bit(in_features=2048, out_features=2048, bias=False)\n          (k_proj): Linear4bit(in_features=2048, out_features=256, bias=False)\n          (v_proj): Linear4bit(in_features=2048, out_features=256, bias=False)\n          (o_proj): Linear4bit(in_features=2048, out_features=2048, bias=False)\n          (rotary_emb): GemmaRotaryEmbedding()\n        )\n        (mlp): GemmaMLP(\n          (gate_proj): Linear4bit(in_features=2048, out_features=16384, bias=False)\n          (up_proj): Linear4bit(in_features=2048, out_features=16384, bias=False)\n          (down_proj): Linear4bit(in_features=16384, out_features=2048, bias=False)\n          (act_fn): GELUActivation()\n        )\n        (input_layernorm): GemmaRMSNorm()\n        (post_attention_layernorm): GemmaRMSNorm()\n      )\n    )\n    (norm): GemmaRMSNorm()\n  )\n  (lm_head): Linear(in_features=2048, out_features=256000, bias=False)\n)"},"metadata":{}}]},{"cell_type":"markdown","source":"### Setting generating Text with the Gemma Model\n\nThis code provides a simple function to generate text using the Gemma model. The Gemma model, a variant of large language models, excels in generating human-like text based on a given prompt. This function utilizes both a model and tokenizer from the Gemma architecture, formatting the output in a specific template for clarity and consistency.","metadata":{}},{"cell_type":"code","source":"# Define a template for formatting instructions and responses.\n# This template will be used to format the text data in a LLM structure.\ntemplate = \"Instruction:\\n{instruction}\\n\\nResponse:\\n{response}\"","metadata":{"execution":{"iopub.status.busy":"2024-04-13T17:18:55.882371Z","iopub.execute_input":"2024-04-13T17:18:55.883004Z","iopub.status.idle":"2024-04-13T17:18:55.891999Z","shell.execute_reply.started":"2024-04-13T17:18:55.882969Z","shell.execute_reply":"2024-04-13T17:18:55.891110Z"},"trusted":true},"execution_count":42,"outputs":[]},{"cell_type":"code","source":"def generate_response(model, tokenizer, prompt, device, max_new_tokens=128):\n    \"\"\"\n    This function generates a response to a given prompt using a specified model and tokenizer.\n\n    Parameters:\n    - model (PreTrainedModel): The machine learning model pre-trained for text generation.\n    - tokenizer (PreTrainedTokenizer): A tokenizer for converting text into a format the model understands.\n    - prompt (str): The initial text prompt to generate a response for.\n    - device (torch.device): The computing device (CPU or GPU) the model should use for calculations.\n    - max_new_tokens (int, optional): The maximum number of new tokens to generate. Defaults to 128.\n\n    Returns:\n    - str: The text generated in response to the prompt.\n    \"\"\"\n    # Convert the prompt into a format the model can understand using the tokenizer.\n    # The result is also moved to the specified computing device.\n    inputs = tokenizer(prompt, return_tensors='pt', padding=True, truncation=True).to(device)\n\n    # Generate a response based on the tokenized prompt.\n    outputs = model.generate(**inputs, num_return_sequences=1, max_new_tokens=max_new_tokens)\n\n    # Convert the generated tokens back into readable text.\n    text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n    \n    # Extract and return the response text. Here, it assumes the response is formatted as \"Response: [generated text]\".\n    response_text = text.split(\"Response:\")[1]\n    \n    return response_text","metadata":{"execution":{"iopub.status.busy":"2024-04-13T17:18:55.893206Z","iopub.execute_input":"2024-04-13T17:18:55.893501Z","iopub.status.idle":"2024-04-13T17:18:55.904199Z","shell.execute_reply.started":"2024-04-13T17:18:55.893477Z","shell.execute_reply":"2024-04-13T17:18:55.903396Z"},"trusted":true},"execution_count":43,"outputs":[]},{"cell_type":"markdown","source":"# 6. Q & A Results Before Finetuning\n\nBefore we start the finetuning process, let's see how the Gemma model performs out of the box on our dataset. This section will show you how to run a simple question-answering test.","metadata":{}},{"cell_type":"code","source":"question = \"What is the difference between an expression and an operator?\"\n\nprompt = template.format(\n    instruction=question,\n    response=\"\",\n)\n\nresponse_text = generate_response(model, tokenizer, prompt, device, 128)\n\nMarkdown(response_text)","metadata":{"execution":{"iopub.status.busy":"2024-04-13T17:18:55.905250Z","iopub.execute_input":"2024-04-13T17:18:55.905530Z","iopub.status.idle":"2024-04-13T17:19:02.044786Z","shell.execute_reply.started":"2024-04-13T17:18:55.905507Z","shell.execute_reply":"2024-04-13T17:19:02.043783Z"},"trusted":true},"execution_count":44,"outputs":[{"name":"stderr","text":"Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n","output_type":"stream"},{"execution_count":44,"output_type":"execute_result","data":{"text/plain":"<IPython.core.display.Markdown object>","text/markdown":"\nSure. Here's the difference between an expression and an operator:\n\n**Expression:**\n\n* An expression is a combination of numbers, variables, operators, and literals that is evaluated to a single value.\n* It is a mathematical or logical statement that represents a numerical value or a truth value.\n* An expression can be a single value, a complex expression, or a compound expression.\n\n**Operator:**\n\n* An operator is a symbol or character that is used to combine or modify the values of other operands.\n* It is not evaluated to a specific value and is not a numerical expression.\n* Operators are used"},"metadata":{}}]},{"cell_type":"markdown","source":"# 7. Applying Gemma LoRA\n\nIn this Session, we'll be applying the LoRA (**Low-Rank Adaptation**) technique to the **Gemma model**, a method designed to make fine-tuning large models like Gemma both **fast and efficient**. LoRA, a part of **PEFT** (**Parameter Efficient Fine-Tuning**), focuses on updating specific parts of a pre-trained model by only training a select few dense layers. This drastically cuts down on the computational demands and GPU memory needs, all without adding any extra time to the inference process. Here's what makes LoRA so powerful for our purposes:\n\n<center><img src=\"https://cdn-lfs.huggingface.co/datasets/huggingface/documentation-images/4313422c5f2755897fb8ddfc5b99251358f679647ec0f2d120a3f1ff060defe7?response-content-disposition=inline%3B+filename*%3DUTF-8%27%27lora_diagram.png%3B+filename%3D%22lora_diagram.png%22%3B&response-content-type=image%2Fpng&Expires=1713275384&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTcxMzI3NTM4NH19LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2RuLWxmcy5odWdnaW5nZmFjZS5jby9kYXRhc2V0cy9odWdnaW5nZmFjZS9kb2N1bWVudGF0aW9uLWltYWdlcy80MzEzNDIyYzVmMjc1NTg5N2ZiOGRkZmM1Yjk5MjUxMzU4ZjY3OTY0N2VjMGYyZDEyMGEzZjFmZjA2MGRlZmU3P3Jlc3BvbnNlLWNvbnRlbnQtZGlzcG9zaXRpb249KiZyZXNwb25zZS1jb250ZW50LXR5cGU9KiJ9XX0_&Signature=NAlgCQRn6ktvkOq8WpJkP7DyBvC3ta3Z5gGREWKvLDGQLYpypCszzucGL7nFdzirC4Py9CkgAgkAwbtGAkBU0JvbDVqxIAK9SzpX34xyFmoERdHqH2sQUh17cZ42f60MU9E%7E209I%7Ec6HgUNponN8lhoQzn0jEKYvkzsVsVUPu4OuYONDx4C1tywJIDovcKZCqEQY7f9-OjEKjLPr-CkNymcE%7Eprd83SMPThprA3HVl4gmMbCslQgUM8mM5imHcFxozdbzgD1Mb0U%7El7THXSeBWXdpGdZIBjbJSwJBEEMBtlVbbKtncPTrZWUjrrq03EJJSB7Cc8IA%7EgtJ3cbUerDGw__&Key-Pair-Id=KVTP0A1DKRTAX\" width=\"500\"><br/>\nPaper: <a href=\"https://arxiv.org/abs/2106.09685\">LoRA: Low-Rank Adaptation of Large Language Models</a></center>\n\n- **Dramatically reduces the number of parameters** needed, by up to **10,000 times**.\n- **Cuts down GPU memory usage** by **three times**.\n- **Maintains quick inference times** with **no additional latency**.\n\nThe essence of PEFT, and by extension LoRA, is to enhance a model's performance using minimal resources, focusing on fine-tuning a handful of parameters for specific tasks. This technique is particularly advantageous as it:\n  \n- Optimizes rank decomposition matrices, maintaining the original model weights while adding optimized low-rank weights **A** and **B**.\n- Allows for up to **threefold reductions** in both time and computational costs.\n- Enables easy swapping of the LoRA module (weights **A** and **B**) according to the task at hand, lowering storage requirements and avoiding any increase in inference time.\n\nWhen applied specifically to **Transformer architectures**, targeting **attention weights** and keeping MLP modules static, LoRA significantly enhances the model's efficiency. For instance, in GPT-3 175B models, it:\n  \n- **Reduces VRAM usage** from **1.2TB to 350GB**.\n- **Lowers checkpoint size** from **350GB to 35MB**.\n- **Boosts training speed** by approximately **25%**.\n\nBy integrating LoRA into Gemma, we aim to streamline the model's fine-tuning process in this Session, making it quicker and more resource-efficient, without compromising on performance.","metadata":{}},{"cell_type":"code","source":"# LoRA configuration: Sets up the parameters for Low-Rank Adaptation, which is a method for efficient fine-tuning of transformers.\nlora_config = LoraConfig(\n    r = 8,  # Rank of the adaptation matrices. A lower rank means fewer parameters to train.\n    target_modules = [\"q_proj\", \"o_proj\", \"k_proj\", \"v_proj\",\n                      \"gate_proj\", \"up_proj\", \"down_proj\"],  # Transformer modules to apply LoRA.\n    task_type = \"CAUSAL_LM\",  # The type of task, here it is causal language modeling.\n)","metadata":{"execution":{"iopub.status.busy":"2024-04-13T17:19:02.046069Z","iopub.execute_input":"2024-04-13T17:19:02.046698Z","iopub.status.idle":"2024-04-13T17:19:02.051888Z","shell.execute_reply.started":"2024-04-13T17:19:02.046662Z","shell.execute_reply":"2024-04-13T17:19:02.050874Z"},"trusted":true},"execution_count":45,"outputs":[]},{"cell_type":"markdown","source":"# 8. Training Gemma\n\nNow that everything is set up, it's time to finetune the Gemma model on your data. This section will guide you through the training process, including setting up your training loop and selecting the right hyperparameters.","metadata":{}},{"cell_type":"code","source":"def formatting_func(example):\n    \"\"\"\n    Formats a given example (a dictionary containing question and answer) using the predefined template.\n    \n    Parameters:\n    - example (dict): A dictionary with keys corresponding to the columns of the dataset, such as 'question' and 'answer'.\n    \n    Returns:\n    - list: A list containing a single formatted string that combines the instruction and the response.\n    \"\"\"\n    # Add the phrase to verify training success and format the text using the template and the specific example's instruction and response.\n    line = template.format(instruction=example[question_column], response=example[answer_column])\n    return [line]\n","metadata":{"execution":{"iopub.status.busy":"2024-04-13T17:19:02.052920Z","iopub.execute_input":"2024-04-13T17:19:02.053215Z","iopub.status.idle":"2024-04-13T17:19:02.068689Z","shell.execute_reply.started":"2024-04-13T17:19:02.053190Z","shell.execute_reply":"2024-04-13T17:19:02.067788Z"},"trusted":true},"execution_count":46,"outputs":[]},{"cell_type":"code","source":"# Setup for the trainer object that will handle fine-tuning of the model.\ntrainer = SFTTrainer(\n    model=model,  # The pre-trained model to fine-tune.\n    train_dataset=dataset,  # The dataset used for training.\n    max_seq_length=512,  # The maximum sequence length for the model inputs.\n    args=TrainingArguments(  # Arguments for training setup.\n        per_device_train_batch_size=1,  # Batch size per device (e.g., GPU).\n        gradient_accumulation_steps=4,  # Number of steps to accumulate gradients before updating model weights.\n        warmup_steps=5,  # Number of steps to gradually increase the learning rate at the beginning of training.\n        max_steps=30,  # Total number of training steps to perform.\n        learning_rate=2e-4,  # Learning rate for the optimizer.\n        fp16=False,  # Whether to use 16-bit floating point precision for training. False means 32-bit is used.\n        logging_steps=1,  # How often to log training information.\n        output_dir=\"outputs\",  # Directory where training outputs will be saved.\n        optim=\"paged_adamw_8bit\"  # The optimizer to use, with 8-bit precision for efficiency.\n    ),\n    peft_config=lora_config,  # The LoRA configuration for efficient fine-tuning.\n    formatting_func=formatting_func,  # The function to format the dataset examples.\n)\n","metadata":{"execution":{"iopub.status.busy":"2024-04-13T17:19:02.069881Z","iopub.execute_input":"2024-04-13T17:19:02.070337Z","iopub.status.idle":"2024-04-13T17:19:04.066929Z","shell.execute_reply.started":"2024-04-13T17:19:02.070304Z","shell.execute_reply":"2024-04-13T17:19:04.065995Z"},"trusted":true},"execution_count":47,"outputs":[{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/210 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e189016cd8794374a618b286ade594c2"}},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/trl/trainer/sft_trainer.py:294: UserWarning: You passed a tokenizer with `padding_side` not equal to `right` to the SFTTrainer. This might lead to some unexpected behaviour due to overflow issues when training a model in half-precision. You might consider adding `tokenizer.padding_side = 'right'` to your code.\n  warnings.warn(\n","output_type":"stream"}]},{"cell_type":"code","source":"# train the model to the processed data.\ntrainer.train()","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2024-04-13T17:19:04.068476Z","iopub.execute_input":"2024-04-13T17:19:04.068824Z","iopub.status.idle":"2024-04-13T17:19:46.369697Z","shell.execute_reply.started":"2024-04-13T17:19:04.068791Z","shell.execute_reply":"2024-04-13T17:19:46.368790Z"},"trusted":true},"execution_count":48,"outputs":[{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='30' max='30' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [30/30 00:40, Epoch 30/30]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>0.311200</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>0.311200</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>0.297600</td>\n    </tr>\n    <tr>\n      <td>4</td>\n      <td>0.271200</td>\n    </tr>\n    <tr>\n      <td>5</td>\n      <td>0.236500</td>\n    </tr>\n    <tr>\n      <td>6</td>\n      <td>0.201100</td>\n    </tr>\n    <tr>\n      <td>7</td>\n      <td>0.168300</td>\n    </tr>\n    <tr>\n      <td>8</td>\n      <td>0.142600</td>\n    </tr>\n    <tr>\n      <td>9</td>\n      <td>0.120400</td>\n    </tr>\n    <tr>\n      <td>10</td>\n      <td>0.101500</td>\n    </tr>\n    <tr>\n      <td>11</td>\n      <td>0.085200</td>\n    </tr>\n    <tr>\n      <td>12</td>\n      <td>0.071300</td>\n    </tr>\n    <tr>\n      <td>13</td>\n      <td>0.060500</td>\n    </tr>\n    <tr>\n      <td>14</td>\n      <td>0.051900</td>\n    </tr>\n    <tr>\n      <td>15</td>\n      <td>0.043500</td>\n    </tr>\n    <tr>\n      <td>16</td>\n      <td>0.035800</td>\n    </tr>\n    <tr>\n      <td>17</td>\n      <td>0.030600</td>\n    </tr>\n    <tr>\n      <td>18</td>\n      <td>0.026000</td>\n    </tr>\n    <tr>\n      <td>19</td>\n      <td>0.021400</td>\n    </tr>\n    <tr>\n      <td>20</td>\n      <td>0.017400</td>\n    </tr>\n    <tr>\n      <td>21</td>\n      <td>0.014600</td>\n    </tr>\n    <tr>\n      <td>22</td>\n      <td>0.011600</td>\n    </tr>\n    <tr>\n      <td>23</td>\n      <td>0.010400</td>\n    </tr>\n    <tr>\n      <td>24</td>\n      <td>0.008900</td>\n    </tr>\n    <tr>\n      <td>25</td>\n      <td>0.008500</td>\n    </tr>\n    <tr>\n      <td>26</td>\n      <td>0.007100</td>\n    </tr>\n    <tr>\n      <td>27</td>\n      <td>0.007000</td>\n    </tr>\n    <tr>\n      <td>28</td>\n      <td>0.006500</td>\n    </tr>\n    <tr>\n      <td>29</td>\n      <td>0.006400</td>\n    </tr>\n    <tr>\n      <td>30</td>\n      <td>0.006300</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"execution_count":48,"output_type":"execute_result","data":{"text/plain":"TrainOutput(global_step=30, training_loss=0.08974544471129775, metrics={'train_runtime': 41.8676, 'train_samples_per_second': 2.866, 'train_steps_per_second': 0.717, 'total_flos': 183554172518400.0, 'train_loss': 0.08974544471129775, 'epoch': 30.0})"},"metadata":{}}]},{"cell_type":"markdown","source":"# 9. Q&A Results After Finetuning\n\nAfter training, let's see how much our Gemma model has improved. We'll rerun the question-answering test and compare the results to the pre-finetuning performance.","metadata":{}},{"cell_type":"code","source":"question = \"What is the difference between an expression and an operator?\"\n\nprompt = template.format(\n    instruction=question,\n    response=\"\",\n)\n\nresponse_text = generate_response(trainer.model, tokenizer, prompt, device, 128)\n\nMarkdown(response_text)","metadata":{"execution":{"iopub.status.busy":"2024-04-13T17:19:46.370828Z","iopub.execute_input":"2024-04-13T17:19:46.371158Z","iopub.status.idle":"2024-04-13T17:19:55.489935Z","shell.execute_reply.started":"2024-04-13T17:19:46.371132Z","shell.execute_reply":"2024-04-13T17:19:55.489009Z"},"trusted":true},"execution_count":49,"outputs":[{"execution_count":49,"output_type":"execute_result","data":{"text/plain":"<IPython.core.display.Markdown object>","text/markdown":"\nSure. Here's the difference between an expression and an operator:\n\n**Expression:**\n\n* An expression is a combination of one or more operands (numbers, variables, functions, etc.) connected by operators.\n* It tells the computer what to do or calculate.\n* An expression can be a single value or a complex expression.\n\n**Operator:**\n\n* An operator is a symbol used to combine or manipulate operands.\n* It tells the computer how to perform the operation.\n* An operator can be a binary operator (e.g., +, -, *, /) or a unary operator (e.g.,"},"metadata":{}}]},{"cell_type":"markdown","source":"**Although** the performance of the Gemma model was already outstanding, it might appear that there is not a significant difference after training. However, the value of this notebook lies in providing a comprehensive learning method for beginners. This is of great importance, and through this notebook, Gemma can also learn about topics it was previously unfamiliar with.","metadata":{}},{"cell_type":"markdown","source":"# 10. Conclusion\n\nIn this beginner-friendly notebook, we've outlined the process of fine-tuning the Gemma model, a Large Language Model (LLM), specifically for Python Q&A generation. Starting from data loading and preprocessing, we've demonstrated how to train the Gemma model effectively, even for those new to working with LLMs.\n\nWe leveraged the Dataset_Python_Question_Answer, featuring hundreds of Python programming questions and answers, to train and refine the Gemma model's capabilities in generating accurate Q&As. This journey, while introductory, underscores the potential and straightforward path to engaging with LLMs through the Gemma model.\n\nAchieving the best performance with the Gemma model (or any LLM) generally requires training with more extensive datasets and over more epochs. Future enhancements could include integrating Retrieval-Augmented Generation (RAG) and Direct Preference Optimization (DPO) training techniques, offering a way to further improve the model by incorporating external knowledge bases for more precise and relevant responses.\n\nUltimately, this notebook is designed to make the Gemma model approachable for beginners, illustrating that straightforward steps can unlock the potential of LLMs for diverse domain-specific tasks. It encourages users to experiment with the Gemma model across various fields, broadening the scope of its application and enhancing its utility.","metadata":{}},{"cell_type":"markdown","source":"<b>If you find this notebook useful, please consider upvoting it.</b> \n   \n<b>This will help others find it and encourage me to write more code, which benefits everyone.</b>","metadata":{}}]}