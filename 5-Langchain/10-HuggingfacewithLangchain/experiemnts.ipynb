{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hugging Face x LangChain : A new partner package in LangChain\n",
    "langchain_huggingface, a partner package in LangChain jointly maintained by Hugging Face and LangChain. This new Python package is designed to bring the power of the latest development of Hugging Face into LangChain and keep it up to date."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: langchain_huggingface in c:\\users\\asus\\desktop\\gen-ai_from_scratch\\5-langchain\\venv\\lib\\site-packages (0.3.0)\n",
      "Requirement already satisfied: langchain-core<1.0.0,>=0.3.65 in c:\\users\\asus\\desktop\\gen-ai_from_scratch\\5-langchain\\venv\\lib\\site-packages (from langchain_huggingface) (0.3.68)\n",
      "Requirement already satisfied: tokenizers>=0.19.1 in c:\\users\\asus\\desktop\\gen-ai_from_scratch\\5-langchain\\venv\\lib\\site-packages (from langchain_huggingface) (0.21.2)\n",
      "Requirement already satisfied: huggingface-hub>=0.30.2 in c:\\users\\asus\\desktop\\gen-ai_from_scratch\\5-langchain\\venv\\lib\\site-packages (from langchain_huggingface) (0.33.2)\n",
      "Requirement already satisfied: langsmith>=0.3.45 in c:\\users\\asus\\desktop\\gen-ai_from_scratch\\5-langchain\\venv\\lib\\site-packages (from langchain-core<1.0.0,>=0.3.65->langchain_huggingface) (0.4.4)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in c:\\users\\asus\\desktop\\gen-ai_from_scratch\\5-langchain\\venv\\lib\\site-packages (from langchain-core<1.0.0,>=0.3.65->langchain_huggingface) (9.1.2)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in c:\\users\\asus\\desktop\\gen-ai_from_scratch\\5-langchain\\venv\\lib\\site-packages (from langchain-core<1.0.0,>=0.3.65->langchain_huggingface) (1.33)\n",
      "Requirement already satisfied: PyYAML>=5.3 in c:\\users\\asus\\desktop\\gen-ai_from_scratch\\5-langchain\\venv\\lib\\site-packages (from langchain-core<1.0.0,>=0.3.65->langchain_huggingface) (6.0.2)\n",
      "Requirement already satisfied: packaging<25,>=23.2 in c:\\users\\asus\\desktop\\gen-ai_from_scratch\\5-langchain\\venv\\lib\\site-packages (from langchain-core<1.0.0,>=0.3.65->langchain_huggingface) (24.2)\n",
      "Requirement already satisfied: typing-extensions>=4.7 in c:\\users\\asus\\desktop\\gen-ai_from_scratch\\5-langchain\\venv\\lib\\site-packages (from langchain-core<1.0.0,>=0.3.65->langchain_huggingface) (4.14.1)\n",
      "Requirement already satisfied: pydantic>=2.7.4 in c:\\users\\asus\\desktop\\gen-ai_from_scratch\\5-langchain\\venv\\lib\\site-packages (from langchain-core<1.0.0,>=0.3.65->langchain_huggingface) (2.11.7)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in c:\\users\\asus\\desktop\\gen-ai_from_scratch\\5-langchain\\venv\\lib\\site-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.65->langchain_huggingface) (3.0.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\asus\\desktop\\gen-ai_from_scratch\\5-langchain\\venv\\lib\\site-packages (from huggingface-hub>=0.30.2->langchain_huggingface) (3.18.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\asus\\desktop\\gen-ai_from_scratch\\5-langchain\\venv\\lib\\site-packages (from huggingface-hub>=0.30.2->langchain_huggingface) (2025.5.1)\n",
      "Requirement already satisfied: requests in c:\\users\\asus\\desktop\\gen-ai_from_scratch\\5-langchain\\venv\\lib\\site-packages (from huggingface-hub>=0.30.2->langchain_huggingface) (2.32.4)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in c:\\users\\asus\\desktop\\gen-ai_from_scratch\\5-langchain\\venv\\lib\\site-packages (from huggingface-hub>=0.30.2->langchain_huggingface) (4.67.1)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in c:\\users\\asus\\desktop\\gen-ai_from_scratch\\5-langchain\\venv\\lib\\site-packages (from langsmith>=0.3.45->langchain-core<1.0.0,>=0.3.65->langchain_huggingface) (0.28.1)\n",
      "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in c:\\users\\asus\\desktop\\gen-ai_from_scratch\\5-langchain\\venv\\lib\\site-packages (from langsmith>=0.3.45->langchain-core<1.0.0,>=0.3.65->langchain_huggingface) (3.10.18)\n",
      "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in c:\\users\\asus\\desktop\\gen-ai_from_scratch\\5-langchain\\venv\\lib\\site-packages (from langsmith>=0.3.45->langchain-core<1.0.0,>=0.3.65->langchain_huggingface) (1.0.0)\n",
      "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in c:\\users\\asus\\desktop\\gen-ai_from_scratch\\5-langchain\\venv\\lib\\site-packages (from langsmith>=0.3.45->langchain-core<1.0.0,>=0.3.65->langchain_huggingface) (0.23.0)\n",
      "Requirement already satisfied: anyio in c:\\users\\asus\\desktop\\gen-ai_from_scratch\\5-langchain\\venv\\lib\\site-packages (from httpx<1,>=0.23.0->langsmith>=0.3.45->langchain-core<1.0.0,>=0.3.65->langchain_huggingface) (4.9.0)\n",
      "Requirement already satisfied: certifi in c:\\users\\asus\\desktop\\gen-ai_from_scratch\\5-langchain\\venv\\lib\\site-packages (from httpx<1,>=0.23.0->langsmith>=0.3.45->langchain-core<1.0.0,>=0.3.65->langchain_huggingface) (2025.6.15)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\asus\\desktop\\gen-ai_from_scratch\\5-langchain\\venv\\lib\\site-packages (from httpx<1,>=0.23.0->langsmith>=0.3.45->langchain-core<1.0.0,>=0.3.65->langchain_huggingface) (1.0.9)\n",
      "Requirement already satisfied: idna in c:\\users\\asus\\desktop\\gen-ai_from_scratch\\5-langchain\\venv\\lib\\site-packages (from httpx<1,>=0.23.0->langsmith>=0.3.45->langchain-core<1.0.0,>=0.3.65->langchain_huggingface) (3.10)\n",
      "Requirement already satisfied: h11>=0.16 in c:\\users\\asus\\desktop\\gen-ai_from_scratch\\5-langchain\\venv\\lib\\site-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith>=0.3.45->langchain-core<1.0.0,>=0.3.65->langchain_huggingface) (0.16.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\asus\\desktop\\gen-ai_from_scratch\\5-langchain\\venv\\lib\\site-packages (from pydantic>=2.7.4->langchain-core<1.0.0,>=0.3.65->langchain_huggingface) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in c:\\users\\asus\\desktop\\gen-ai_from_scratch\\5-langchain\\venv\\lib\\site-packages (from pydantic>=2.7.4->langchain-core<1.0.0,>=0.3.65->langchain_huggingface) (2.33.2)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in c:\\users\\asus\\desktop\\gen-ai_from_scratch\\5-langchain\\venv\\lib\\site-packages (from pydantic>=2.7.4->langchain-core<1.0.0,>=0.3.65->langchain_huggingface) (0.4.1)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\asus\\desktop\\gen-ai_from_scratch\\5-langchain\\venv\\lib\\site-packages (from requests->huggingface-hub>=0.30.2->langchain_huggingface) (3.4.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\asus\\desktop\\gen-ai_from_scratch\\5-langchain\\venv\\lib\\site-packages (from requests->huggingface-hub>=0.30.2->langchain_huggingface) (2.5.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\asus\\desktop\\gen-ai_from_scratch\\5-langchain\\venv\\lib\\site-packages (from tqdm>=4.42.1->huggingface-hub>=0.30.2->langchain_huggingface) (0.4.6)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in c:\\users\\asus\\desktop\\gen-ai_from_scratch\\5-langchain\\venv\\lib\\site-packages (from anyio->httpx<1,>=0.23.0->langsmith>=0.3.45->langchain-core<1.0.0,>=0.3.65->langchain_huggingface) (1.3.0)\n",
      "Requirement already satisfied: sniffio>=1.1 in c:\\users\\asus\\desktop\\gen-ai_from_scratch\\5-langchain\\venv\\lib\\site-packages (from anyio->httpx<1,>=0.23.0->langsmith>=0.3.45->langchain-core<1.0.0,>=0.3.65->langchain_huggingface) (1.3.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install langchain_huggingface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: huggingface_hub in c:\\users\\asus\\desktop\\gen-ai_from_scratch\\5-langchain\\venv\\lib\\site-packages (0.33.2)\n",
      "Requirement already satisfied: filelock in c:\\users\\asus\\desktop\\gen-ai_from_scratch\\5-langchain\\venv\\lib\\site-packages (from huggingface_hub) (3.18.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\asus\\desktop\\gen-ai_from_scratch\\5-langchain\\venv\\lib\\site-packages (from huggingface_hub) (2025.5.1)\n",
      "Requirement already satisfied: packaging>=20.9 in c:\\users\\asus\\desktop\\gen-ai_from_scratch\\5-langchain\\venv\\lib\\site-packages (from huggingface_hub) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\asus\\desktop\\gen-ai_from_scratch\\5-langchain\\venv\\lib\\site-packages (from huggingface_hub) (6.0.2)\n",
      "Requirement already satisfied: requests in c:\\users\\asus\\desktop\\gen-ai_from_scratch\\5-langchain\\venv\\lib\\site-packages (from huggingface_hub) (2.32.4)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in c:\\users\\asus\\desktop\\gen-ai_from_scratch\\5-langchain\\venv\\lib\\site-packages (from huggingface_hub) (4.67.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\asus\\desktop\\gen-ai_from_scratch\\5-langchain\\venv\\lib\\site-packages (from huggingface_hub) (4.14.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\asus\\desktop\\gen-ai_from_scratch\\5-langchain\\venv\\lib\\site-packages (from tqdm>=4.42.1->huggingface_hub) (0.4.6)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\asus\\desktop\\gen-ai_from_scratch\\5-langchain\\venv\\lib\\site-packages (from requests->huggingface_hub) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\asus\\desktop\\gen-ai_from_scratch\\5-langchain\\venv\\lib\\site-packages (from requests->huggingface_hub) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\asus\\desktop\\gen-ai_from_scratch\\5-langchain\\venv\\lib\\site-packages (from requests->huggingface_hub) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\asus\\desktop\\gen-ai_from_scratch\\5-langchain\\venv\\lib\\site-packages (from requests->huggingface_hub) (2025.6.15)\n"
     ]
    }
   ],
   "source": [
    "## API Call\n",
    "!pip install huggingface_hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HuggingFaceEndpoint\n",
    "#### How to Access HuggingFace Models with API\n",
    "There are also two ways to use this class. You can specify the model with the repo_id parameter. Those endpoints use the serverless API, which is particularly beneficial to people using pro accounts or enterprise hub. Still, regular users can already have access to a fair amount of request by connecting with their HF token in the environment where they are executing the code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from getpass import getpass\n",
    "\n",
    "HUGGINGFACEHUB_API_TOKEN = getpass()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_huggingface import HuggingFaceEndpoint\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<generator object HfApi.list_models at 0x000001CE8AD346D0>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ASUS\\Desktop\\Gen-Ai_from_Scratch\\5-Langchain\\venv\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from huggingface_hub import list_models\n",
    "\n",
    "models = list_models()\n",
    "print(models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "HuggingFaceEndpoint(repo_id='mistralai/Mistral-Nemo-Base-2407', provider='auto', huggingfacehub_api_token='', max_new_tokens=150, temperature=0.7, stop_sequences=[], server_kwargs={}, model_kwargs={}, model='mistralai/Mistral-Nemo-Base-2407', client=<InferenceClient(model='mistralai/Mistral-Nemo-Base-2407', timeout=120)>, async_client=<InferenceClient(model='mistralai/Mistral-Nemo-Base-2407', timeout=120)>)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "repo_id=\"mistralai/Mistral-Nemo-Base-2407\"\n",
    "llm=HuggingFaceEndpoint(repo_id=repo_id,temperature=0.7, max_new_tokens=150,huggingfacehub_api_token=HUGGINGFACEHUB_API_TOKEN, provider=\"auto\"\n",
    ")\n",
    "llm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Why Deep Learning? How to train a Deep Learning Model? Is Deep Learning same as Machine Learning? Why Deep Learning? What is the difference between Machine Learning and Deep Learning? Is Deep Learning same as Artificial Intelligence? These are the questions that I will answer in this article.\n",
      "\n",
      "## What is Deep Learning?\n",
      "\n",
      "> Deep Learning is a subset of machine learning in artificial intelligence (AI) that has networks capable of learning unsupervised from data that is unstructured or unlabeled. Also known as deep neural learning or deep neural network.\n",
      "\n",
      "### Why Deep Learning?\n",
      "\n",
      "Deep learning uses artificial neural networks that are loosely modeled on the way our brain works. The most popular types of deep learning are convolutional neural networks (CNN) and recurrent neural networks (RNN\n"
     ]
    }
   ],
   "source": [
    "print(llm.invoke(\"What is Deep Learning?\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' Why is it so popular now?\\n\\nDeep learning is a subset of machine learning that uses artificial neural networks to solve complex problems. Deep learning has been around for decades, but it has only recently become popular due to advancements in computer hardware and algorithms.\\n\\nOne of the main reasons deep learning is so popular now is because it can be used to solve a wide variety of problems. For example, deep learning can be used for image recognition, natural language processing, and even game playing. Deep learning is also popular because it can be used to create new applications and services.\\n\\nAnother reason deep learning is so popular now is because it is relatively easy to use. Deep learning algorithms are often easier to understand and implement than traditional machine learning algorithms. This makes deep learning'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# os.getenv(\"HF_TOKEN\")\n",
    "repo_id=\"mistralai/Mistral-Nemo-Base-2407\"\n",
    "llm=HuggingFaceEndpoint(repo_id=repo_id,temperature=0.7, max_new_tokens=150,huggingfacehub_api_token=os.getenv(\"HF_TOKEN\"), provider=\"auto\"\n",
    ")\n",
    "llm.invoke(\"What is Deep Learning?\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import huggingface_hub\n",
    "# huggingface-cli --login\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Model meta-llama/Meta-Llama-3-8B-Instruct is not supported for task text-generation and provider novita. Supported task: conversational.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 9\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mlangchain_huggingface\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m HuggingFaceEndpoint\n\u001b[0;32m      3\u001b[0m llm \u001b[38;5;241m=\u001b[39m HuggingFaceEndpoint(\n\u001b[0;32m      4\u001b[0m     repo_id\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmeta-llama/Meta-Llama-3-8B-Instruct\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m      5\u001b[0m     task\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext-generation\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m      6\u001b[0m     max_new_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m100\u001b[39m,\n\u001b[0;32m      7\u001b[0m     do_sample\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m      8\u001b[0m )\n\u001b[1;32m----> 9\u001b[0m \u001b[43mllm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mHugging Face is\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\ASUS\\Desktop\\Gen-Ai_from_Scratch\\5-Langchain\\venv\\lib\\site-packages\\langchain_core\\language_models\\llms.py:389\u001b[0m, in \u001b[0;36mBaseLLM.invoke\u001b[1;34m(self, input, config, stop, **kwargs)\u001b[0m\n\u001b[0;32m    378\u001b[0m \u001b[38;5;129m@override\u001b[39m\n\u001b[0;32m    379\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21minvoke\u001b[39m(\n\u001b[0;32m    380\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    385\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[0;32m    386\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mstr\u001b[39m:\n\u001b[0;32m    387\u001b[0m     config \u001b[38;5;241m=\u001b[39m ensure_config(config)\n\u001b[0;32m    388\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (\n\u001b[1;32m--> 389\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgenerate_prompt(\n\u001b[0;32m    390\u001b[0m             [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_convert_input(\u001b[38;5;28minput\u001b[39m)],\n\u001b[0;32m    391\u001b[0m             stop\u001b[38;5;241m=\u001b[39mstop,\n\u001b[0;32m    392\u001b[0m             callbacks\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcallbacks\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m    393\u001b[0m             tags\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtags\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m    394\u001b[0m             metadata\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmetadata\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m    395\u001b[0m             run_name\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_name\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m    396\u001b[0m             run_id\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_id\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[0;32m    397\u001b[0m             \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m    398\u001b[0m         )\n\u001b[0;32m    399\u001b[0m         \u001b[38;5;241m.\u001b[39mgenerations[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m    400\u001b[0m         \u001b[38;5;241m.\u001b[39mtext\n\u001b[0;32m    401\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\ASUS\\Desktop\\Gen-Ai_from_Scratch\\5-Langchain\\venv\\lib\\site-packages\\langchain_core\\language_models\\llms.py:766\u001b[0m, in \u001b[0;36mBaseLLM.generate_prompt\u001b[1;34m(self, prompts, stop, callbacks, **kwargs)\u001b[0m\n\u001b[0;32m    757\u001b[0m \u001b[38;5;129m@override\u001b[39m\n\u001b[0;32m    758\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mgenerate_prompt\u001b[39m(\n\u001b[0;32m    759\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    763\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[0;32m    764\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m LLMResult:\n\u001b[0;32m    765\u001b[0m     prompt_strings \u001b[38;5;241m=\u001b[39m [p\u001b[38;5;241m.\u001b[39mto_string() \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m prompts]\n\u001b[1;32m--> 766\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgenerate(prompt_strings, stop\u001b[38;5;241m=\u001b[39mstop, callbacks\u001b[38;5;241m=\u001b[39mcallbacks, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\ASUS\\Desktop\\Gen-Ai_from_Scratch\\5-Langchain\\venv\\lib\\site-packages\\langchain_core\\language_models\\llms.py:973\u001b[0m, in \u001b[0;36mBaseLLM.generate\u001b[1;34m(self, prompts, stop, callbacks, tags, metadata, run_name, run_id, **kwargs)\u001b[0m\n\u001b[0;32m    958\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcache \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m get_llm_cache() \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcache \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m:\n\u001b[0;32m    959\u001b[0m     run_managers \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m    960\u001b[0m         callback_manager\u001b[38;5;241m.\u001b[39mon_llm_start(\n\u001b[0;32m    961\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_serialized,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    971\u001b[0m         )\n\u001b[0;32m    972\u001b[0m     ]\n\u001b[1;32m--> 973\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generate_helper(\n\u001b[0;32m    974\u001b[0m         prompts,\n\u001b[0;32m    975\u001b[0m         stop,\n\u001b[0;32m    976\u001b[0m         run_managers,\n\u001b[0;32m    977\u001b[0m         new_arg_supported\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mbool\u001b[39m(new_arg_supported),\n\u001b[0;32m    978\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m    979\u001b[0m     )\n\u001b[0;32m    980\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(missing_prompts) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m    981\u001b[0m     run_managers \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m    982\u001b[0m         callback_managers[idx]\u001b[38;5;241m.\u001b[39mon_llm_start(\n\u001b[0;32m    983\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_serialized,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    990\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m missing_prompt_idxs\n\u001b[0;32m    991\u001b[0m     ]\n",
      "File \u001b[1;32mc:\\Users\\ASUS\\Desktop\\Gen-Ai_from_Scratch\\5-Langchain\\venv\\lib\\site-packages\\langchain_core\\language_models\\llms.py:792\u001b[0m, in \u001b[0;36mBaseLLM._generate_helper\u001b[1;34m(self, prompts, stop, run_managers, new_arg_supported, **kwargs)\u001b[0m\n\u001b[0;32m    781\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_generate_helper\u001b[39m(\n\u001b[0;32m    782\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    783\u001b[0m     prompts: \u001b[38;5;28mlist\u001b[39m[\u001b[38;5;28mstr\u001b[39m],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    788\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[0;32m    789\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m LLMResult:\n\u001b[0;32m    790\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    791\u001b[0m         output \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m--> 792\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generate(\n\u001b[0;32m    793\u001b[0m                 prompts,\n\u001b[0;32m    794\u001b[0m                 stop\u001b[38;5;241m=\u001b[39mstop,\n\u001b[0;32m    795\u001b[0m                 \u001b[38;5;66;03m# TODO: support multiple run managers\u001b[39;00m\n\u001b[0;32m    796\u001b[0m                 run_manager\u001b[38;5;241m=\u001b[39mrun_managers[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m run_managers \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    797\u001b[0m                 \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m    798\u001b[0m             )\n\u001b[0;32m    799\u001b[0m             \u001b[38;5;28;01mif\u001b[39;00m new_arg_supported\n\u001b[0;32m    800\u001b[0m             \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generate(prompts, stop\u001b[38;5;241m=\u001b[39mstop)\n\u001b[0;32m    801\u001b[0m         )\n\u001b[0;32m    802\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    803\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m run_manager \u001b[38;5;129;01min\u001b[39;00m run_managers:\n",
      "File \u001b[1;32mc:\\Users\\ASUS\\Desktop\\Gen-Ai_from_Scratch\\5-Langchain\\venv\\lib\\site-packages\\langchain_core\\language_models\\llms.py:1547\u001b[0m, in \u001b[0;36mLLM._generate\u001b[1;34m(self, prompts, stop, run_manager, **kwargs)\u001b[0m\n\u001b[0;32m   1544\u001b[0m new_arg_supported \u001b[38;5;241m=\u001b[39m inspect\u001b[38;5;241m.\u001b[39msignature(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call)\u001b[38;5;241m.\u001b[39mparameters\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_manager\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   1545\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m prompt \u001b[38;5;129;01min\u001b[39;00m prompts:\n\u001b[0;32m   1546\u001b[0m     text \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m-> 1547\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call(prompt, stop\u001b[38;5;241m=\u001b[39mstop, run_manager\u001b[38;5;241m=\u001b[39mrun_manager, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1548\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m new_arg_supported\n\u001b[0;32m   1549\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call(prompt, stop\u001b[38;5;241m=\u001b[39mstop, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1550\u001b[0m     )\n\u001b[0;32m   1551\u001b[0m     generations\u001b[38;5;241m.\u001b[39mappend([Generation(text\u001b[38;5;241m=\u001b[39mtext)])\n\u001b[0;32m   1552\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m LLMResult(generations\u001b[38;5;241m=\u001b[39mgenerations)\n",
      "File \u001b[1;32mc:\\Users\\ASUS\\Desktop\\Gen-Ai_from_Scratch\\5-Langchain\\venv\\lib\\site-packages\\langchain_huggingface\\llms\\huggingface_endpoint.py:312\u001b[0m, in \u001b[0;36mHuggingFaceEndpoint._call\u001b[1;34m(self, prompt, stop, run_manager, **kwargs)\u001b[0m\n\u001b[0;32m    310\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m completion\n\u001b[0;32m    311\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 312\u001b[0m     response_text \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclient\u001b[38;5;241m.\u001b[39mtext_generation(\n\u001b[0;32m    313\u001b[0m         prompt\u001b[38;5;241m=\u001b[39mprompt,\n\u001b[0;32m    314\u001b[0m         model\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel,\n\u001b[0;32m    315\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39minvocation_params,\n\u001b[0;32m    316\u001b[0m     )\n\u001b[0;32m    318\u001b[0m     \u001b[38;5;66;03m# Maybe the generation has stopped at one of the stop sequences:\u001b[39;00m\n\u001b[0;32m    319\u001b[0m     \u001b[38;5;66;03m# then we remove this stop sequence from the end of the generated text\u001b[39;00m\n\u001b[0;32m    320\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m stop_seq \u001b[38;5;129;01min\u001b[39;00m invocation_params[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstop\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n",
      "File \u001b[1;32mc:\\Users\\ASUS\\Desktop\\Gen-Ai_from_Scratch\\5-Langchain\\venv\\lib\\site-packages\\huggingface_hub\\inference\\_client.py:2297\u001b[0m, in \u001b[0;36mInferenceClient.text_generation\u001b[1;34m(self, prompt, details, stream, model, adapter_id, best_of, decoder_input_details, do_sample, frequency_penalty, grammar, max_new_tokens, repetition_penalty, return_full_text, seed, stop, stop_sequences, temperature, top_k, top_n_tokens, top_p, truncate, typical_p, watermark)\u001b[0m\n\u001b[0;32m   2295\u001b[0m model_id \u001b[38;5;241m=\u001b[39m model \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\n\u001b[0;32m   2296\u001b[0m provider_helper \u001b[38;5;241m=\u001b[39m get_provider_helper(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprovider, task\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext-generation\u001b[39m\u001b[38;5;124m\"\u001b[39m, model\u001b[38;5;241m=\u001b[39mmodel_id)\n\u001b[1;32m-> 2297\u001b[0m request_parameters \u001b[38;5;241m=\u001b[39m \u001b[43mprovider_helper\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprepare_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   2298\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprompt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2299\u001b[0m \u001b[43m    \u001b[49m\u001b[43mparameters\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparameters\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2300\u001b[0m \u001b[43m    \u001b[49m\u001b[43mextra_payload\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstream\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2301\u001b[0m \u001b[43m    \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2302\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2303\u001b[0m \u001b[43m    \u001b[49m\u001b[43mapi_key\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2304\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2306\u001b[0m \u001b[38;5;66;03m# Handle errors separately for more precise error messages\u001b[39;00m\n\u001b[0;32m   2307\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\ASUS\\Desktop\\Gen-Ai_from_Scratch\\5-Langchain\\venv\\lib\\site-packages\\huggingface_hub\\inference\\_providers\\_common.py:93\u001b[0m, in \u001b[0;36mTaskProviderHelper.prepare_request\u001b[1;34m(self, inputs, parameters, headers, model, api_key, extra_payload)\u001b[0m\n\u001b[0;32m     90\u001b[0m api_key \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_prepare_api_key(api_key)\n\u001b[0;32m     92\u001b[0m \u001b[38;5;66;03m# mapped model from HF model ID\u001b[39;00m\n\u001b[1;32m---> 93\u001b[0m provider_mapping_info \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_prepare_mapping_info\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     95\u001b[0m \u001b[38;5;66;03m# default HF headers + user headers (to customize in subclasses)\u001b[39;00m\n\u001b[0;32m     96\u001b[0m headers \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_prepare_headers(headers, api_key)\n",
      "File \u001b[1;32mc:\\Users\\ASUS\\Desktop\\Gen-Ai_from_Scratch\\5-Langchain\\venv\\lib\\site-packages\\huggingface_hub\\inference\\_providers\\_common.py:162\u001b[0m, in \u001b[0;36mTaskProviderHelper._prepare_mapping_info\u001b[1;34m(self, model)\u001b[0m\n\u001b[0;32m    159\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModel \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m is not supported by provider \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprovider\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    161\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m provider_mapping\u001b[38;5;241m.\u001b[39mtask \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtask:\n\u001b[1;32m--> 162\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    163\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModel \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m is not supported for task \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtask\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m and provider \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprovider\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    164\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSupported task: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mprovider_mapping\u001b[38;5;241m.\u001b[39mtask\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    165\u001b[0m     )\n\u001b[0;32m    167\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m provider_mapping\u001b[38;5;241m.\u001b[39mstatus \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstaging\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    168\u001b[0m     logger\u001b[38;5;241m.\u001b[39mwarning(\n\u001b[0;32m    169\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModel \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m is in staging mode for provider \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprovider\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. Meant for test purposes only.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    170\u001b[0m     )\n",
      "\u001b[1;31mValueError\u001b[0m: Model meta-llama/Meta-Llama-3-8B-Instruct is not supported for task text-generation and provider novita. Supported task: conversational."
     ]
    }
   ],
   "source": [
    "# from langchain_huggingface import HuggingFaceEndpoint\n",
    "\n",
    "# llm = HuggingFaceEndpoint(\n",
    "#     repo_id=\"meta-llama/Meta-Llama-3-8B-Instruct\",\n",
    "#     task=\"text-generation\",\n",
    "#     max_new_tokens=100,\n",
    "#     do_sample=False,\n",
    "# )\n",
    "# llm.invoke(\"Hugging Face is\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_variables=['question'] input_types={} partial_variables={} template='\\nQuestion:{question}\\nAnswer:Lets think step by step.\\n'\n"
     ]
    }
   ],
   "source": [
    "from langchain import PromptTemplate,LLMChain\n",
    "template=\"\"\"\n",
    "Question:{question}\n",
    "Answer:Lets think step by step.\n",
    "\"\"\"\n",
    "prompt=PromptTemplate(template=template,input_variables=[\"question\"])\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ASUS\\AppData\\Local\\Temp\\ipykernel_24476\\3735178259.py:1: LangChainDeprecationWarning: The class `LLMChain` was deprecated in LangChain 0.1.17 and will be removed in 1.0. Use :meth:`~RunnableSequence, e.g., `prompt | llm`` instead.\n",
      "  llm_chain=LLMChain(llm=llm,prompt=prompt)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'? Dhoni or Tendulkar? India or Sri Lanka? It’s a question that is going to keep people arguing for years. This article is not about who won the World Cup. It’s about what the World Cup 2011 did for India.\\n\\nThe last time India won the World Cup, I was in 5th grade. I was in a convent school and all I could see was Indian flags, saffron, green and white everywhere. I didn’t understand what the World Cup was and I was too young to follow the game. But I do remember how the streets were deserted and everyone was glued to the television screens. And I remember how we celebrated the victory, even though we didn’t know what we'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm_chain=LLMChain(llm=llm,prompt=prompt)\n",
    "llm.invoke(\"Who won the cricket World up 2011\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "bge models best open source embedding models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ASUS\\AppData\\Local\\Temp\\ipykernel_24476\\2593296450.py:6: LangChainDeprecationWarning: The class `HuggingFaceBgeEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEmbeddings``.\n",
      "  hf = HuggingFaceBgeEmbeddings(\n",
      "c:\\Users\\ASUS\\Desktop\\Gen-Ai_from_Scratch\\5-Langchain\\venv\\lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\ASUS\\.cache\\huggingface\\hub\\models--BAAI--bge-small-en. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.embeddings import HuggingFaceBgeEmbeddings\n",
    "\n",
    "model_name = \"BAAI/bge-small-en\"\n",
    "model_kwargs = {\"device\": \"cpu\"}\n",
    "encode_kwargs = {\"normalize_embeddings\": True}\n",
    "hf = HuggingFaceBgeEmbeddings(\n",
    "    model_name=model_name, model_kwargs=model_kwargs, encode_kwargs=encode_kwargs\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching 2 files:   0%|          | 0/2 [00:00<?, ?it/s]"
     ]
    }
   ],
   "source": [
    "from langchain_huggingface import HuggingFacePipeline\n",
    "\n",
    "llm = HuggingFacePipeline.from_model_id(\n",
    "    model_id=\"microsoft/Phi-3-mini-4k-instruct\",\n",
    "    task=\"text-generation\",\n",
    "    pipeline_kwargs={\n",
    "        \"max_new_tokens\": 100,\n",
    "        \"top_k\": 50,\n",
    "        \"temperature\": 0.1,\n",
    "    },\n",
    ")\n",
    "llm.invoke(\"Hugging Face is\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding = hf.embed_query(\"hi this is harrison\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[-0.02841656468808651,\n",
       " 0.012183268554508686,\n",
       " 0.027443984523415565,\n",
       " -0.05482865497469902,\n",
       " 0.024238867685198784,\n",
       " 0.0007662881398573518,\n",
       " 0.06783364713191986,\n",
       " 0.016348354518413544,\n",
       " -0.018950767815113068,\n",
       " 0.012542914599180222,\n",
       " 0.021564999595284462,\n",
       " -0.08793038874864578,\n",
       " 0.0006460413569584489,\n",
       " 0.03327082097530365,\n",
       " 0.005463749170303345,\n",
       " -0.06037645414471626,\n",
       " 0.05042261630296707,\n",
       " 0.004434795584529638,\n",
       " 0.0009598496835678816,\n",
       " 0.0017405670369043946,\n",
       " 0.003298831405118108,\n",
       " 0.03167250379920006,\n",
       " -0.04880749061703682,\n",
       " -0.044819142669439316,\n",
       " 0.07132111489772797,\n",
       " -0.007510872557759285,\n",
       " -0.0011259716702625155,\n",
       " -0.015801170840859413,\n",
       " -0.029402365908026695,\n",
       " -0.17224565148353577,\n",
       " -0.03189518675208092,\n",
       " -0.001629168982617557,\n",
       " 0.018104983493685722,\n",
       " 0.0153154032304883,\n",
       " -0.020729567855596542,\n",
       " -0.008872982114553452,\n",
       " -0.0012822651769965887,\n",
       " 0.027276871725916862,\n",
       " -0.010114259086549282,\n",
       " 0.012621616013348103,\n",
       " -0.007077881135046482,\n",
       " -0.016693193465471268,\n",
       " 0.040855828672647476,\n",
       " 0.023938357830047607,\n",
       " -0.0200815387070179,\n",
       " 0.028681132942438126,\n",
       " -0.019400736317038536,\n",
       " -0.014618180692195892,\n",
       " 0.0173796359449625,\n",
       " 0.0041640931740403175,\n",
       " 0.06415650993585587,\n",
       " 0.04768305644392967,\n",
       " 0.0018365010619163513,\n",
       " -8.072093623923138e-05,\n",
       " 0.016596803441643715,\n",
       " 0.011124194599688053,\n",
       " 0.0696943998336792,\n",
       " 0.051820479333400726,\n",
       " 0.05568530410528183,\n",
       " 0.05551542714238167,\n",
       " 0.0005039449315518141,\n",
       " 0.0418706014752388,\n",
       " -0.15344089269638062,\n",
       " 0.05180782824754715,\n",
       " 0.006689794361591339,\n",
       " -0.031670715659856796,\n",
       " -0.00910500343888998,\n",
       " -0.05160471796989441,\n",
       " 0.042508576065301895,\n",
       " 0.028200050815939903,\n",
       " -0.010748136788606644,\n",
       " 0.022405795753002167,\n",
       " 0.04439550265669823,\n",
       " 0.004115518182516098,\n",
       " 0.01899847388267517,\n",
       " -0.004357174038887024,\n",
       " 0.04762765020132065,\n",
       " 0.011824645102024078,\n",
       " 0.008164611645042896,\n",
       " 0.008177300915122032,\n",
       " -0.009698744863271713,\n",
       " -0.01426029484719038,\n",
       " 0.011409717611968517,\n",
       " -0.07362115383148193,\n",
       " -0.054395221173763275,\n",
       " -0.05703963339328766,\n",
       " -0.0036085746251046658,\n",
       " 0.0026660864241421223,\n",
       " 0.023782452568411827,\n",
       " 0.01537621021270752,\n",
       " -0.07020371407270432,\n",
       " -0.03130034729838371,\n",
       " -0.0031142805237323046,\n",
       " -0.015812169760465622,\n",
       " -0.037913978099823,\n",
       " -0.02592192403972149,\n",
       " 0.018168406561017036,\n",
       " -0.03882461413741112,\n",
       " -0.056745074689388275,\n",
       " 0.5792059898376465,\n",
       " -0.05278834328055382,\n",
       " 0.020716384053230286,\n",
       " 0.06794390827417374,\n",
       " -0.04541653394699097,\n",
       " 0.011642461642622948,\n",
       " -0.02157175913453102,\n",
       " 0.020341727882623672,\n",
       " -0.027448948472738266,\n",
       " -0.04558895155787468,\n",
       " -0.02944357879459858,\n",
       " -0.023662520572543144,\n",
       " -0.03431525453925133,\n",
       " 0.0019388606306165457,\n",
       " -0.07095140218734741,\n",
       " 0.034556325525045395,\n",
       " -0.030558940023183823,\n",
       " 0.03907861188054085,\n",
       " -0.029707303270697594,\n",
       " -0.0008282861672341824,\n",
       " -0.012159358710050583,\n",
       " -0.018272804096341133,\n",
       " 0.025486506521701813,\n",
       " -0.004461637232452631,\n",
       " 0.016335321590304375,\n",
       " 0.01912650652229786,\n",
       " -0.05483203008770943,\n",
       " 0.027635984122753143,\n",
       " -0.004757650196552277,\n",
       " 0.05900171399116516,\n",
       " -0.0016944739036262035,\n",
       " 0.008015000261366367,\n",
       " -0.037726834416389465,\n",
       " -0.09893040359020233,\n",
       " -0.022574398666620255,\n",
       " -0.037604689598083496,\n",
       " -0.002169860526919365,\n",
       " 0.0032446151599287987,\n",
       " -0.019202526658773422,\n",
       " -0.00863122008740902,\n",
       " -0.04802309721708298,\n",
       " 0.00869671069085598,\n",
       " -0.09516111761331558,\n",
       " -0.03496047854423523,\n",
       " -0.04360799118876457,\n",
       " -0.0003440282307565212,\n",
       " -0.010173655115067959,\n",
       " -0.030999546870589256,\n",
       " 0.024309691041707993,\n",
       " -0.02040202170610428,\n",
       " 0.031139401718974113,\n",
       " 0.0008811111329123378,\n",
       " 0.013916490599513054,\n",
       " -0.0311962328851223,\n",
       " -0.037154048681259155,\n",
       " 0.004029621835798025,\n",
       " 0.014799792319536209,\n",
       " 0.04318893328309059,\n",
       " 0.03875483572483063,\n",
       " 0.013852010481059551,\n",
       " 0.019797861576080322,\n",
       " 0.010267049074172974,\n",
       " -0.005434098187834024,\n",
       " -0.014299226924777031,\n",
       " 0.027637822553515434,\n",
       " 0.009802641347050667,\n",
       " -0.13550284504890442,\n",
       " -0.017139768227934837,\n",
       " 0.017617113888263702,\n",
       " 0.023132257163524628,\n",
       " 0.0017590099014341831,\n",
       " 0.030889414250850677,\n",
       " 0.0399186909198761,\n",
       " -0.013684151694178581,\n",
       " 0.024816518649458885,\n",
       " 0.05405019223690033,\n",
       " 0.017761152237653732,\n",
       " -0.018475066870450974,\n",
       " 0.02595539763569832,\n",
       " -0.0063775149174034595,\n",
       " -0.01658732257783413,\n",
       " 0.037848006933927536,\n",
       " -0.027290036901831627,\n",
       " -0.0528457909822464,\n",
       " -0.038033176213502884,\n",
       " 0.05191105976700783,\n",
       " -0.00755709782242775,\n",
       " -0.03180527687072754,\n",
       " 0.013284189626574516,\n",
       " -0.027723705396056175,\n",
       " 0.05630652233958244,\n",
       " 0.0030418927781283855,\n",
       " 0.05332484096288681,\n",
       " -0.0579112246632576,\n",
       " -0.011325825937092304,\n",
       " -0.031171994283795357,\n",
       " 0.02560870349407196,\n",
       " 0.033890608698129654,\n",
       " -0.0010284590534865856,\n",
       " 0.01586488075554371,\n",
       " 0.01059521920979023,\n",
       " -0.027037804946303368,\n",
       " -0.0009308407315984368,\n",
       " -0.04815221577882767,\n",
       " 0.028179233893752098,\n",
       " 0.010320608504116535,\n",
       " 0.06662959605455399,\n",
       " -0.01655818335711956,\n",
       " -0.004431380890309811,\n",
       " 0.03823424503207207,\n",
       " -0.023408209905028343,\n",
       " -0.035581737756729126,\n",
       " -0.05829070881009102,\n",
       " -0.011181486770510674,\n",
       " -0.017684556543827057,\n",
       " -0.016141297295689583,\n",
       " -0.03424534946680069,\n",
       " -0.025139542296528816,\n",
       " 0.03939666599035263,\n",
       " -0.023658229038119316,\n",
       " -0.007725008763372898,\n",
       " -0.005098927300423384,\n",
       " -0.03523437678813934,\n",
       " -0.014076863415539265,\n",
       " -0.2232602834701538,\n",
       " -0.031471364200115204,\n",
       " -0.0012906071497127414,\n",
       " -0.0017199907451868057,\n",
       " -0.007846049033105373,\n",
       " -0.058023251593112946,\n",
       " 0.046174563467502594,\n",
       " 0.02455265074968338,\n",
       " 0.07320840656757355,\n",
       " 0.017268355935811996,\n",
       " 0.047612063586711884,\n",
       " 0.01347330305725336,\n",
       " -0.005516010336577892,\n",
       " -0.014357834123075008,\n",
       " -0.009674319997429848,\n",
       " 0.04878249391913414,\n",
       " 0.03053811378777027,\n",
       " -0.024993976578116417,\n",
       " 0.021486258134245872,\n",
       " 0.017639806494116783,\n",
       " 0.053138937801122665,\n",
       " 0.013484999537467957,\n",
       " -0.023226004093885422,\n",
       " -0.02140398696064949,\n",
       " 0.026075372472405434,\n",
       " 0.002029179595410824,\n",
       " 0.12753744423389435,\n",
       " 0.08316835761070251,\n",
       " 0.04408949986100197,\n",
       " -0.026703620329499245,\n",
       " 0.005522000603377819,\n",
       " -0.009294911287724972,\n",
       " 0.020074333995580673,\n",
       " -0.09684176743030548,\n",
       " -0.024703925475478172,\n",
       " 0.02508697845041752,\n",
       " 0.002088631736114621,\n",
       " -0.044894028455019,\n",
       " -0.0786113515496254,\n",
       " -0.004376342985779047,\n",
       " -0.06590455025434494,\n",
       " 0.014689408242702484,\n",
       " -0.057641852647066116,\n",
       " -0.07152028381824493,\n",
       " -0.06232648715376854,\n",
       " 0.0034316396340727806,\n",
       " -0.046065498143434525,\n",
       " 0.045300863683223724,\n",
       " -0.026762252673506737,\n",
       " 0.034010931849479675,\n",
       " 0.04547380656003952,\n",
       " -0.028179218992590904,\n",
       " 0.005011801607906818,\n",
       " 0.009630811400711536,\n",
       " -0.030305616557598114,\n",
       " -0.036124806851148605,\n",
       " -0.013626936823129654,\n",
       " -0.032653722912073135,\n",
       " -0.04467751830816269,\n",
       " 0.010642170906066895,\n",
       " -0.027486328035593033,\n",
       " -0.024565106257796288,\n",
       " -0.024747760966420174,\n",
       " 0.05361957848072052,\n",
       " 0.020789971575140953,\n",
       " 0.019468478858470917,\n",
       " 0.053241219371557236,\n",
       " -0.014002474024891853,\n",
       " 0.021243248134851456,\n",
       " -0.049573253840208054,\n",
       " -0.008522587828338146,\n",
       " 0.007852853275835514,\n",
       " -0.05719394236803055,\n",
       " -0.027550632134079933,\n",
       " 0.005300886929035187,\n",
       " 0.040072910487651825,\n",
       " 0.019597908481955528,\n",
       " -0.04519734904170036,\n",
       " 0.0324358195066452,\n",
       " -0.012342404574155807,\n",
       " 0.034314416348934174,\n",
       " 0.02110210247337818,\n",
       " 0.03984646871685982,\n",
       " 0.03166377171874046,\n",
       " -0.03359021991491318,\n",
       " 0.03164784982800484,\n",
       " -0.0033045082818716764,\n",
       " 0.004641848616302013,\n",
       " 0.03758932277560234,\n",
       " -0.05924459919333458,\n",
       " 0.007028403226286173,\n",
       " 0.003808701876550913,\n",
       " -0.02578888274729252,\n",
       " -0.02120332419872284,\n",
       " 0.022691210731863976,\n",
       " -0.021772993728518486,\n",
       " -0.27963775396347046,\n",
       " 0.007267419248819351,\n",
       " 0.0210720207542181,\n",
       " 0.045197442173957825,\n",
       " -0.02053447626531124,\n",
       " 0.024313705042004585,\n",
       " -0.0006137107848189771,\n",
       " -0.01185702532529831,\n",
       " -0.032967787235975266,\n",
       " 0.0358431376516819,\n",
       " 0.0312817357480526,\n",
       " 0.06373955309391022,\n",
       " 0.04654783755540848,\n",
       " -0.014470523223280907,\n",
       " 0.01586974412202835,\n",
       " 0.03397127613425255,\n",
       " 0.018059568479657173,\n",
       " 0.002298766979947686,\n",
       " 0.0165498536080122,\n",
       " -0.02171487547457218,\n",
       " -0.03485997021198273,\n",
       " -0.0008649277733638883,\n",
       " 0.15126042068004608,\n",
       " -0.024536756798624992,\n",
       " 0.030671212822198868,\n",
       " -0.007318181451410055,\n",
       " -0.006135466508567333,\n",
       " 0.06415144354104996,\n",
       " 0.016021493822336197,\n",
       " -0.03636444732546806,\n",
       " 0.01989860273897648,\n",
       " -0.021172314882278442,\n",
       " 0.04829411953687668,\n",
       " -0.044780977070331573,\n",
       " 0.047633808106184006,\n",
       " 0.0007749417563900352,\n",
       " -0.0059279585257172585,\n",
       " 0.061542678624391556,\n",
       " 0.023968392983078957,\n",
       " 0.01330502424389124,\n",
       " 0.022684510797262192,\n",
       " 0.014538082294166088,\n",
       " -0.052159082144498825,\n",
       " -0.032749660313129425,\n",
       " 0.08583346009254456,\n",
       " -0.003724851878359914,\n",
       " 0.0013493997976183891,\n",
       " 0.040919914841651917,\n",
       " 0.011659649200737476,\n",
       " 0.05843618884682655,\n",
       " -0.022286174818873405,\n",
       " -0.011520706117153168,\n",
       " 0.004705706145614386,\n",
       " 0.047182608395814896,\n",
       " -0.0019179129740223289,\n",
       " 0.033009376376867294,\n",
       " -0.03505055233836174,\n",
       " -0.020736567676067352,\n",
       " -0.009222187101840973,\n",
       " 0.01461828127503395,\n",
       " 0.006456071976572275,\n",
       " 0.0010978570207953453,\n",
       " 0.010223980993032455,\n",
       " 0.0853721871972084,\n",
       " 0.038839519023895264]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
